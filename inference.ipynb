{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ™¶ä½“ç»“æ„ç”Ÿæˆæ¨ç† Notebook - è¿­ä»£ä¼˜åŒ–ç‰ˆ\n",
    "\n",
    "æœ¬notebookå®ç°è¿­ä»£ä¼˜åŒ–çš„æ¨ç†æµç¨‹ï¼š\n",
    "\n",
    "## æ ¸å¿ƒæµç¨‹\n",
    "1. **åˆå§‹æ¨ç†**ï¼šæ¨¡å‹æ¨ç†ç”Ÿæˆåˆå§‹submission.csv\n",
    "2. **è´¨é‡è¯„ä¼°**ï¼šä½¿ç”¨RWPæŒ‡æ ‡è¯„ä¼°PXRDåŒ¹é…è´¨é‡\n",
    "3. **è¿­ä»£ä¼˜åŒ–å¾ªç¯**ï¼š\n",
    "   - è´¨é‡ä¸å¥½çš„æ ·æœ¬è¿›è¡Œåå¤„ç†ï¼ˆèƒ½é‡ä¼˜åŒ–â†’Rietveldç²¾ä¿®ï¼‰\n",
    "   - å¦‚æœè´¨é‡æ”¹å–„åˆ™æ›´æ–°submission.csv\n",
    "   - ä»ä¸æ»¡è¶³è¦æ±‚çš„æ‰¹é‡é‡æ–°ç”Ÿæˆ\n",
    "4. **ç»ˆæ­¢æ¡ä»¶**ï¼š\n",
    "   - æ€»è¿è¡Œæ—¶é—´è¶…è¿‡5å°æ—¶\n",
    "   - å•ä¸ªæ ·æœ¬å°è¯•æ¬¡æ•°è¶…é™\n",
    "   - æ‰€æœ‰æ ·æœ¬æ»¡è¶³è´¨é‡è¦æ±‚\n",
    "\n",
    "**æ³¨æ„**: è¿™ä¸ªnotebookè®¾è®¡ä¸ºå¯ç›´æ¥åœ¨æ¯”èµ›ç¯å¢ƒè¿è¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥å¿…è¦çš„åº“å’Œè®¾ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## CFG (Classifier-Free Guidance) ä½¿ç”¨è¯´æ˜\n\næœ¬notebooké»˜è®¤ä½¿ç”¨ **cfm_cfg** æµæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå®ƒæ”¯æŒåŠ¨æ€è°ƒèŠ‚ç”Ÿæˆè´¨é‡ï¼š\n\n### æ ¸å¿ƒå‚æ•°\n- **CFG_GUIDANCE_SCALE** (é»˜è®¤1.5): æ§åˆ¶æ¡ä»¶å¼•å¯¼å¼ºåº¦\n  - `1.0`: æ ‡å‡†æ¡ä»¶ç”Ÿæˆï¼ˆæ— å¢å¼ºï¼‰\n  - `>1.0`: å¢å¼ºæ¡ä»¶æ§åˆ¶ï¼ˆæ›´ç²¾ç¡®åŒ¹é…PXRDï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆï¼‰\n  - `<1.0`: å¢åŠ å¤šæ ·æ€§ï¼ˆæ›´å¤šæ¢ç´¢ï¼Œä½†å¯èƒ½åç¦»ç›®æ ‡ï¼‰\n\n### è‡ªé€‚åº”ç­–ç•¥\n- **è¿­ä»£æ—©æœŸ** (1-2è½®): æ ‡å‡†å¼•å¯¼å¼ºåº¦ï¼Œå¹³è¡¡æ¢ç´¢\n- **è¿­ä»£ä¸­æœŸ** (3-5è½®): å¢å¼ºå¼•å¯¼å¼ºåº¦ï¼Œç²¾ç¡®åŒ¹é…\n- **è¿­ä»£åæœŸ** (>5è½®): é™ä½å¼•å¯¼å¼ºåº¦ï¼Œå¢åŠ å¤šæ ·æ€§\n- **å›°éš¾æ ·æœ¬**: æ ¹æ®åŸå­æ•°é‡è‡ªåŠ¨è°ƒèŠ‚å¼•å¯¼å¼ºåº¦\n\n### ä½¿ç”¨ç¤ºä¾‹\n```python\n# æ‰‹åŠ¨è°ƒèŠ‚å•ä¸ªæ ·æœ¬çš„å¼•å¯¼å¼ºåº¦\nstructure = model.flow.sample(conditions, guidance_scale=2.0)  # å¼ºå¼•å¯¼\n\n# æ‰¹é‡ç”Ÿæˆæ—¶ä½¿ç”¨ä¸åŒç­–ç•¥\nstructures, scales = generate_crystal_structures_batch_cfg(\n    df, model, normalizer,\n    guidance_scale=1.5,      # å›ºå®šå¼•å¯¼å¼ºåº¦\n    adaptive_mode=True       # æˆ–ä½¿ç”¨è‡ªé€‚åº”æ¨¡å¼\n)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchç‰ˆæœ¬: 2.8.0+cu128\n",
      "CUDAå¯ç”¨: True\n",
      "CUDAè®¾å¤‡: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from pymatgen.core import Structure, Lattice, Composition, Element\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„\n",
    "sys.path.append('.')  # æ·»åŠ æ ¹ç›®å½•\n",
    "sys.path.append('src')\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„æ¨¡å—\n",
    "from src.trainer import CrystalGenerationModule\n",
    "from src.pxrd_simulator import PXRDSimulator\n",
    "from src.normalizer import DataNormalizer\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDAè®¾å¤‡: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. é…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æ•°æ®è·¯å¾„é…ç½®\nDATA_DIR = Path(\"data/A_sample\")  # æ¯”èµ›æ•°æ®ç›®å½•\nCOMPOSITION_FILE = DATA_DIR / \"composition.json\"\nPATTERN_DIR = DATA_DIR / \"pattern\"\n\n# æ¨¡å‹è·¯å¾„ - ä½¿ç”¨å®é™…çš„checkpointè·¯å¾„\nMODEL_PATH = \"outputs/transformer_cfm_20250828_144134/checkpoints/last.ckpt\"\n\n# è¾“å‡ºæ–‡ä»¶ï¼ˆå¿…é¡»åœ¨æ ¹ç›®å½•ï¼‰\nSUBMISSION_FILE = \"submission.csv\"\n\n# CFGæ¨ç†å‚æ•°\nCFG_GUIDANCE_SCALE = 1.5  # é»˜è®¤CFGå¼•å¯¼å¼ºåº¦ï¼ˆ1.0=æ ‡å‡†ï¼Œ>1å¢å¼ºæ¡ä»¶æ§åˆ¶ï¼‰\nCFG_ADAPTIVE_MODE = True  # æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”å¼•å¯¼å¼ºåº¦\nCFG_MIN_SCALE = 0.8  # è‡ªé€‚åº”æ¨¡å¼ä¸‹çš„æœ€å°å¼•å¯¼å¼ºåº¦\nCFG_MAX_SCALE = 2.5  # è‡ªé€‚åº”æ¨¡å¼ä¸‹çš„æœ€å¤§å¼•å¯¼å¼ºåº¦\n\n# ä¼˜åŒ–å‚æ•°\nRWP_THRESHOLD = 0.15  # RWPè´¨é‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºè´¨é‡åˆæ ¼\nMAX_TIME_HOURS = 5  # æœ€å¤§è¿è¡Œæ—¶é—´ï¼ˆå°æ—¶ï¼‰\nMAX_ATTEMPTS_PER_SAMPLE = 10  # æ¯ä¸ªæ ·æœ¬æœ€å¤§å°è¯•æ¬¡æ•°\nBATCH_SIZE = 32  # æ‰¹é‡é‡æ–°ç”Ÿæˆçš„å¤§å°\n\n# è®°å½•å¼€å§‹æ—¶é—´\nSTART_TIME = time.time()\nMAX_RUNTIME = MAX_TIME_HOURS * 3600  # è½¬æ¢ä¸ºç§’\n\nprint(f\"é…ç½®å‚æ•°ï¼š\")\nprint(f\"  æ¨¡å‹è·¯å¾„: {MODEL_PATH}\")\nprint(f\"  æ¨¡å‹å­˜åœ¨: {os.path.exists(MODEL_PATH)}\")\nprint(f\"  æµæ¨¡å‹: cfm_cfg (Classifier-Free Guidance)\")\nprint(f\"  CFGå¼•å¯¼å¼ºåº¦: {CFG_GUIDANCE_SCALE}\")\nprint(f\"  è‡ªé€‚åº”æ¨¡å¼: {CFG_ADAPTIVE_MODE}\")\nif CFG_ADAPTIVE_MODE:\n    print(f\"    å¼•å¯¼å¼ºåº¦èŒƒå›´: [{CFG_MIN_SCALE}, {CFG_MAX_SCALE}]\")\nprint(f\"  RWPé˜ˆå€¼: {RWP_THRESHOLD}\")\nprint(f\"  æœ€å¤§è¿è¡Œæ—¶é—´: {MAX_TIME_HOURS}å°æ—¶\")\nprint(f\"  å•æ ·æœ¬æœ€å¤§å°è¯•: {MAX_ATTEMPTS_PER_SAMPLE}æ¬¡\")\nprint(f\"  å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ•°æ®åŠ è½½å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xy_file(file_path):\n",
    "    \"\"\"è¯»å–.xyæ ¼å¼çš„PXRDæ•°æ®\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                intensity = float(parts[1])\n",
    "                data.append(intensity)\n",
    "    return np.array(data, dtype=np.float32)\n",
    "\n",
    "def parse_composition(comp_str):\n",
    "    \"\"\"è§£æç»„æˆå­—ç¬¦ä¸²ä¸ºåŸå­ç±»å‹å’Œæ•°é‡\"\"\"\n",
    "    comp = Composition(comp_str)\n",
    "    atom_list = []\n",
    "    \n",
    "    for element, count in comp.items():\n",
    "        atomic_num = Element(element).Z\n",
    "        atom_list.extend([atomic_num] * int(count))\n",
    "    \n",
    "    # å¡«å……åˆ°60ç»´\n",
    "    atom_types = np.zeros(60, dtype=np.int32)\n",
    "    atom_types[:len(atom_list)] = atom_list[:60]\n",
    "    \n",
    "    return len(atom_list), atom_types\n",
    "\n",
    "def load_competition_data(data_dir):\n",
    "    \"\"\"åŠ è½½æ¯”èµ›æ ¼å¼æ•°æ®\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # è¯»å–composition\n",
    "    with open(data_dir / \"composition.json\", 'r') as f:\n",
    "        compositions = json.load(f)\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®åˆ—è¡¨\n",
    "    data_list = []\n",
    "    \n",
    "    for sample_id, comp_info in tqdm(compositions.items(), desc=\"åŠ è½½æ•°æ®\"):\n",
    "        # è·å–ç»„æˆä¿¡æ¯\n",
    "        comp_list = comp_info[\"composition\"]\n",
    "        niggli_comp = comp_list[0]\n",
    "        primitive_comp = comp_list[1] if len(comp_list) > 1 else comp_list[0]\n",
    "        \n",
    "        # è§£æåŸå­ä¿¡æ¯\n",
    "        num_atoms, atom_types = parse_composition(niggli_comp)\n",
    "        \n",
    "        # è¯»å–PXRDæ•°æ®\n",
    "        pattern_file = data_dir / \"pattern\" / f\"{sample_id}.xy\"\n",
    "        if pattern_file.exists():\n",
    "            pxrd = read_xy_file(pattern_file)\n",
    "            # ç¡®ä¿é•¿åº¦ä¸º11501\n",
    "            if len(pxrd) < 11501:\n",
    "                pxrd_full = np.zeros(11501, dtype=np.float32)\n",
    "                pxrd_full[:len(pxrd)] = pxrd\n",
    "                pxrd = pxrd_full\n",
    "            elif len(pxrd) > 11501:\n",
    "                pxrd = pxrd[:11501]\n",
    "        else:\n",
    "            pxrd = np.zeros(11501, dtype=np.float32)\n",
    "        \n",
    "        data_list.append({\n",
    "            'id': sample_id,\n",
    "            'niggli_comp': niggli_comp,\n",
    "            'primitive_comp': primitive_comp,\n",
    "            'atom_types': atom_types,\n",
    "            'num_atoms': num_atoms,\n",
    "            'pxrd': pxrd  # è§‚æµ‹çš„PXRDè°±\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. åŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a289b1b519154b3da104435c03145d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "åŠ è½½æ•°æ®:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "åŠ è½½äº† 200 ä¸ªæ ·æœ¬\n",
      "æ•°æ®åˆ—: ['id', 'niggli_comp', 'primitive_comp', 'atom_types', 'num_atoms', 'pxrd']\n",
      "\n",
      "å‰5ä¸ªæ ·æœ¬:\n",
      "       id         niggli_comp  num_atoms\n",
      "0   A-329        Sb3 Sc10 Te7         20\n",
      "1  A-1447       B3 Mg1 N6 Sr4         14\n",
      "2  A-1150  Ba1 Nd1 O6 Os1 Sr1         10\n",
      "3   A-559         Eu1 Ga3 Zn1          5\n",
      "4  A-1956      Ba4 Gd1 Nb1 O8         14\n",
      "\n",
      "åˆå§‹åŒ–äº† 200 ä¸ªæ ·æœ¬çš„çŠ¶æ€è¿½è¸ª\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ¯”èµ›æ•°æ®\n",
    "df = load_competition_data(DATA_DIR)\n",
    "print(f\"\\nåŠ è½½äº† {len(df)} ä¸ªæ ·æœ¬\")\n",
    "print(f\"æ•°æ®åˆ—: {df.columns.tolist()}\")\n",
    "print(f\"\\nå‰5ä¸ªæ ·æœ¬:\")\n",
    "print(df[['id', 'niggli_comp', 'num_atoms']].head())\n",
    "\n",
    "# åˆå§‹åŒ–æ ·æœ¬çŠ¶æ€è¿½è¸ª\n",
    "sample_status = {\n",
    "    sample_id: {\n",
    "        'attempts': 0,\n",
    "        'best_rwp': float('inf'),\n",
    "        'best_structure': None,\n",
    "        'satisfied': False\n",
    "    }\n",
    "    for sample_id in df['id']\n",
    "}\n",
    "\n",
    "print(f\"\\nåˆå§‹åŒ–äº† {len(sample_status)} ä¸ªæ ·æœ¬çš„çŠ¶æ€è¿½è¸ª\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¨¡å‹å’Œæ¨ç†å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åŠ è½½æ¨¡å‹å’Œåˆå§‹åŒ–å·¥å…·\ndef load_model(model_path):\n    \"\"\"\n    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆä½¿ç”¨cfm_cfgæµï¼‰\n    \n    Args:\n        model_path: checkpointæ–‡ä»¶è·¯å¾„\n        \n    Returns:\n        åŠ è½½å¥½çš„Lightningæ¨¡å—\n    \"\"\"\n    print(f\"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}\")\n    \n    # ä»checkpointåŠ è½½æ¨¡å‹\n    model = CrystalGenerationModule.load_from_checkpoint(\n        model_path,\n        map_location='cuda' if torch.cuda.is_available() else 'cpu'\n    )\n    \n    # éªŒè¯æ˜¯å¦ä½¿ç”¨äº†cfm_cfg\n    flow_name = model.hparams.get('flow_name', 'cfm')\n    print(f\"  æ£€æµ‹åˆ°æµæ¨¡å‹: {flow_name}\")\n    \n    # å¦‚æœåŸæ¨¡å‹ä¸æ˜¯cfm_cfgï¼Œå¯ä»¥åŠ¨æ€æ›¿æ¢ï¼ˆå¦‚æœç½‘ç»œå…¼å®¹ï¼‰\n    if flow_name != 'cfm_cfg':\n        print(f\"  âš ï¸ åŸæ¨¡å‹ä½¿ç”¨{flow_name}ï¼Œå°è¯•åˆ‡æ¢åˆ°cfm_cfg...\")\n        from src.flows import build_flow\n        \n        # æ„å»ºcfm_cfgæµï¼Œå¤ç”¨åŸæœ‰ç½‘ç»œ\n        cfg_config = {\n            'sigma_min': 1e-4,\n            'sigma_max': 1.0,\n            'loss_weight_lattice': 2.0,\n            'loss_weight_coords': 1.0,\n            'cfg_prob': 0.1,  # è®­ç»ƒæ—¶çš„dropoutæ¦‚ç‡\n            'cfg_scale': CFG_GUIDANCE_SCALE,  # ä½¿ç”¨é…ç½®çš„å¼•å¯¼å¼ºåº¦\n            'normalize_lattice': True,\n            'normalize_frac_coords': False,\n            'use_global_stats': True,\n        }\n        \n        # æ›¿æ¢flow\n        model.flow = build_flow('cfm_cfg', model.network, cfg_config)\n        print(f\"  âœ… å·²åˆ‡æ¢åˆ°cfm_cfgæµæ¨¡å‹\")\n    \n    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n    model.eval()\n    \n    # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    print(f\"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}\")\n    return model\n\ndef generate_crystal_structures_batch_cfg(samples_df, model, data_normalizer, \n                                         batch_size=32, guidance_scale=None,\n                                         adaptive_mode=False):\n    \"\"\"\n    æ‰¹é‡ç”Ÿæˆæ™¶ä½“ç»“æ„ï¼ˆä½¿ç”¨CFGå¼•å¯¼ï¼‰\n    \n    Args:\n        samples_df: åŒ…å«å¤šä¸ªæ ·æœ¬çš„DataFrame\n        model: è®­ç»ƒå¥½çš„æ¨¡å‹\n        data_normalizer: æ•°æ®å½’ä¸€åŒ–å™¨\n        batch_size: æ‰¹å¤„ç†å¤§å°\n        guidance_scale: CFGå¼•å¯¼å¼ºåº¦ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰\n        adaptive_mode: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”å¼•å¯¼å¼ºåº¦\n    \n    Returns:\n        list of (Structureå¯¹è±¡, ä½¿ç”¨çš„guidance_scale)\n    \"\"\"\n    device = next(model.parameters()).device\n    structures = []\n    scales_used = []\n    \n    # æŒ‰æ‰¹æ¬¡å¤„ç†\n    num_samples = len(samples_df)\n    for batch_start in range(0, num_samples, batch_size):\n        batch_end = min(batch_start + batch_size, num_samples)\n        batch_df = samples_df.iloc[batch_start:batch_end]\n        \n        # å‡†å¤‡æ‰¹æ¬¡æ•°æ®\n        batch = {\n            'comp': torch.tensor(\n                np.stack(batch_df['atom_types'].values), \n                dtype=torch.float32\n            ).to(device),\n            'pxrd': torch.tensor(\n                np.stack(batch_df['pxrd'].values), \n                dtype=torch.float32\n            ).to(device),\n            'num_atoms': torch.tensor(\n                batch_df['num_atoms'].values, \n                dtype=torch.long\n            ).to(device),\n        }\n        \n        # è‡ªé€‚åº”é€‰æ‹©å¼•å¯¼å¼ºåº¦\n        if adaptive_mode:\n            # æ ¹æ®æ ·æœ¬å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å¼•å¯¼å¼ºåº¦\n            # å¤æ‚åº¦å¯ä»¥åŸºäºåŸå­æ•°é‡ã€ç»„æˆå¤æ‚æ€§ç­‰\n            complexities = batch_df['num_atoms'].values / 60.0  # å½’ä¸€åŒ–åˆ°[0,1]\n            batch_scales = CFG_MIN_SCALE + (CFG_MAX_SCALE - CFG_MIN_SCALE) * complexities\n        else:\n            batch_scales = [guidance_scale or CFG_GUIDANCE_SCALE] * len(batch_df)\n        \n        # å¯¹æ¯ä¸ªä¸åŒçš„scaleå€¼åˆ†ç»„å¤„ç†\n        unique_scales = np.unique(batch_scales)\n        \n        for scale in unique_scales:\n            scale_mask = (batch_scales == scale)\n            scale_indices = np.where(scale_mask)[0]\n            \n            if len(scale_indices) == 0:\n                continue\n            \n            # å‡†å¤‡å­æ‰¹æ¬¡\n            sub_batch = {\n                'comp': batch['comp'][scale_indices],\n                'pxrd': batch['pxrd'][scale_indices],\n                'num_atoms': batch['num_atoms'][scale_indices],\n            }\n            \n            # ä½¿ç”¨CFGé‡‡æ ·\n            with torch.no_grad():\n                generated = model.flow.sample(\n                    sub_batch, \n                    guidance_scale=float(scale),  # ä½¿ç”¨å½“å‰çš„å¼•å¯¼å¼ºåº¦\n                    temperature=1.0,\n                    num_steps=50  # å¯ä»¥è°ƒæ•´é‡‡æ ·æ­¥æ•°\n                )  # [sub_batch_size, 63, 3]\n            \n            # åå½’ä¸€åŒ–\n            generated_denorm = data_normalizer.denormalize_z(generated)\n            generated_denorm = generated_denorm.cpu().numpy()\n            \n            # å¤„ç†æ¯ä¸ªæ ·æœ¬\n            for i, local_idx in enumerate(scale_indices):\n                row = batch_df.iloc[local_idx]\n                num_atoms = row.num_atoms\n                \n                # æå–æ™¶æ ¼å’Œåˆ†æ•°åæ ‡\n                single_output = generated_denorm[i]  # [63, 3]\n                lattice_matrix = single_output[:3, :]  # [3, 3]\n                frac_coords = single_output[3:3+num_atoms, :]  # [num_atoms, 3]\n                frac_coords = np.mod(frac_coords, 1.0)\n                \n                # è·å–å…ƒç´ åˆ—è¡¨\n                species = []\n                for j in range(num_atoms):\n                    atomic_num = int(row.atom_types[j])\n                    if atomic_num > 0:\n                        species.append(Element.from_Z(atomic_num))\n                \n                # åˆ›å»ºStructureå¯¹è±¡\n                try:\n                    lattice = Lattice(lattice_matrix)\n                    structure = Structure(\n                        lattice=lattice,\n                        species=species,\n                        coords=frac_coords,\n                        coords_are_cartesian=False\n                    )\n                    structures.append(structure)\n                    scales_used.append(scale)\n                except Exception as e:\n                    # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨éšæœºç»“æ„\n                    structures.append(generate_random_structure(row._asdict()))\n                    scales_used.append(scale)\n    \n    return structures, scales_used\n\ndef generate_crystal_structures_batch(samples_df, model, data_normalizer, batch_size=32):\n    \"\"\"\n    æ‰¹é‡ç”Ÿæˆæ™¶ä½“ç»“æ„ï¼ˆå…¼å®¹æ¥å£ï¼Œä½¿ç”¨é»˜è®¤CFGè®¾ç½®ï¼‰\n    \"\"\"\n    structures, _ = generate_crystal_structures_batch_cfg(\n        samples_df, model, data_normalizer, \n        batch_size=batch_size,\n        guidance_scale=CFG_GUIDANCE_SCALE,\n        adaptive_mode=CFG_ADAPTIVE_MODE\n    )\n    return structures\n\ndef generate_crystal_structure(sample, model, data_normalizer, pxrd_simulator):\n    \"\"\"\n    ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå•ä¸ªæ™¶ä½“ç»“æ„ï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰\n    \n    Args:\n        sample: åŒ…å«pxrdã€atom_typesã€num_atomsç­‰ä¿¡æ¯çš„æ ·æœ¬\n        model: è®­ç»ƒå¥½çš„æ¨¡å‹\n        data_normalizer: æ•°æ®å½’ä¸€åŒ–å™¨\n        pxrd_simulator: PXRDä»¿çœŸå™¨ï¼ˆè¿™é‡Œæœªä½¿ç”¨ï¼‰\n    \n    Returns:\n        Structureå¯¹è±¡\n    \"\"\"\n    # è½¬æ¢ä¸ºDataFrameæ ¼å¼\n    sample_df = pd.DataFrame([sample])\n    structures = generate_crystal_structures_batch(sample_df, model, data_normalizer, batch_size=1)\n    return structures[0] if structures else generate_random_structure(sample)\n\ndef generate_random_structure(sample):\n    \"\"\"\n    ç”Ÿæˆéšæœºæ™¶ä½“ç»“æ„ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰\n    \n    Args:\n        sample: æ ·æœ¬æ•°æ®\n        \n    Returns:\n        Structureå¯¹è±¡\n    \"\"\"\n    num_atoms = sample['num_atoms'] if isinstance(sample, dict) else sample.num_atoms\n    \n    # éšæœºæ™¶æ ¼å‚æ•°\n    a = np.random.uniform(3, 10)\n    b = np.random.uniform(3, 10)\n    c = np.random.uniform(3, 10)\n    alpha = np.random.uniform(60, 120)\n    beta = np.random.uniform(60, 120)\n    gamma = np.random.uniform(60, 120)\n    \n    lattice = Lattice.from_parameters(a, b, c, alpha, beta, gamma)\n    frac_coords = np.random.rand(num_atoms, 3)\n    \n    atom_types = sample['atom_types'] if isinstance(sample, dict) else sample.atom_types\n    species = []\n    for i in range(num_atoms):\n        atomic_num = int(atom_types[i])\n        if atomic_num > 0:\n            species.append(Element.from_Z(atomic_num))\n    \n    return Structure(\n        lattice=lattice,\n        species=species,\n        coords=frac_coords,\n        coords_are_cartesian=False\n    )\n\n# åŠ è½½æ¨¡å‹å’Œåˆå§‹åŒ–å·¥å…·\ntry:\n    model = load_model(MODEL_PATH)\n    data_normalizer = DataNormalizer()\n    pxrd_simulator = PXRDSimulator()\n    print(\"âœ… æ¨¡å‹å’Œå·¥å…·åˆå§‹åŒ–æˆåŠŸ\")\n    print(f\"   ä½¿ç”¨CFGå¼•å¯¼ï¼Œé»˜è®¤å¼ºåº¦: {CFG_GUIDANCE_SCALE}\")\nexcept Exception as e:\n    print(f\"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n    print(\"å°†ä½¿ç”¨éšæœºç”Ÿæˆä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ\")\n    model = None\n    data_normalizer = None\n    pxrd_simulator = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PXRDè®¡ç®—å’Œè´¨é‡è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "def calculate_pxrd(structure, pxrd_simulator=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ™¶ä½“ç»“æ„çš„PXRDè°±\n",
    "    \n",
    "    Args:\n",
    "        structure: pymatgen Structureå¯¹è±¡\n",
    "        pxrd_simulator: PXRDä»¿çœŸå™¨å®ä¾‹\n",
    "    \n",
    "    Returns:\n",
    "        np.array: 11501ç»´çš„PXRDå¼ºåº¦æ•°ç»„\n",
    "    \"\"\"\n",
    "    if pxrd_simulator is None:\n",
    "        # å¦‚æœæ²¡æœ‰æä¾›simulatorï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„\n",
    "        from src.pxrd_simulator import PXRDSimulator\n",
    "        pxrd_simulator = PXRDSimulator()\n",
    "    \n",
    "    try:\n",
    "        # ä½¿ç”¨PXRDSimulatorè®¡ç®—PXRD\n",
    "        x_angles, pxrd_intensities = pxrd_simulator.simulate(structure)\n",
    "        \n",
    "        # pxrd_intensitieså·²ç»æ˜¯11501ç»´\n",
    "        return pxrd_intensities\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PXRDè®¡ç®—å¤±è´¥: {e}\")\n",
    "        # è¿”å›éšæœºPXRDä½œä¸ºå¤‡ç”¨\n",
    "        pxrd_calc = np.random.rand(11501) * 100\n",
    "        pxrd_calc[pxrd_calc < 10] = 0\n",
    "        return pxrd_calc\n",
    "\n",
    "def calculate_pxrd_worker(structure):\n",
    "    \"\"\"ç”¨äºå¤šè¿›ç¨‹çš„PXRDè®¡ç®—workerå‡½æ•°\"\"\"\n",
    "    from src.pxrd_simulator import PXRDSimulator\n",
    "    simulator = PXRDSimulator()\n",
    "    try:\n",
    "        x_angles, pxrd_intensities = simulator.simulate(structure)\n",
    "        return pxrd_intensities\n",
    "    except:\n",
    "        return np.random.rand(11501) * 100\n",
    "\n",
    "def calculate_pxrd_batch(structures, n_workers=4):\n",
    "    \"\"\"\n",
    "    æ‰¹é‡è®¡ç®—PXRDè°±ï¼ˆä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œï¼‰\n",
    "    \n",
    "    Args:\n",
    "        structures: Structureå¯¹è±¡åˆ—è¡¨\n",
    "        n_workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°\n",
    "    \n",
    "    Returns:\n",
    "        list of PXRDæ•°ç»„\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨å¤šè¿›ç¨‹æ± å¹¶è¡Œè®¡ç®—PXRD\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        pxrd_results = list(executor.map(calculate_pxrd_worker, structures))\n",
    "    \n",
    "    return pxrd_results\n",
    "\n",
    "def evaluate_structure_quality(structure, observed_pxrd, pxrd_simulator=None):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°ç”Ÿæˆç»“æ„çš„è´¨é‡\n",
    "    \n",
    "    Args:\n",
    "        structure: ç”Ÿæˆçš„Structureå¯¹è±¡\n",
    "        observed_pxrd: è§‚æµ‹çš„PXRDè°±\n",
    "        pxrd_simulator: PXRDä»¿çœŸå™¨å®ä¾‹\n",
    "    \n",
    "    Returns:\n",
    "        float: RWPå€¼ï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n",
    "    \"\"\"\n",
    "    # è®¡ç®—ç”Ÿæˆç»“æ„çš„PXRD\n",
    "    calculated_pxrd = calculate_pxrd(structure, pxrd_simulator)\n",
    "    \n",
    "    # ç®€å•çš„RWPè®¡ç®—ï¼ˆå¦‚æœæ²¡æœ‰ä¸“é—¨çš„metricsæ¨¡å—ï¼‰\n",
    "    try:\n",
    "        from src.metrics import rwp\n",
    "        rwp_value = rwp(calculated_pxrd, observed_pxrd)\n",
    "    except ImportError:\n",
    "        # å¤‡ç”¨RWPè®¡ç®—\n",
    "        diff = calculated_pxrd - observed_pxrd\n",
    "        weighted_diff = diff * np.sqrt(np.maximum(observed_pxrd, 1e-10))\n",
    "        rwp_value = np.sqrt(np.sum(weighted_diff**2) / np.sum(observed_pxrd**2 + 1e-10))\n",
    "    \n",
    "    return rwp_value\n",
    "\n",
    "def evaluate_structures_batch(structures, observed_pxrds, n_workers=4):\n",
    "    \"\"\"\n",
    "    æ‰¹é‡è¯„ä¼°ç»“æ„è´¨é‡\n",
    "    \n",
    "    Args:\n",
    "        structures: Structureå¯¹è±¡åˆ—è¡¨\n",
    "        observed_pxrds: è§‚æµ‹PXRDåˆ—è¡¨\n",
    "        n_workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°\n",
    "    \n",
    "    Returns:\n",
    "        list of RWPå€¼\n",
    "    \"\"\"\n",
    "    # æ‰¹é‡è®¡ç®—PXRD\n",
    "    calculated_pxrds = calculate_pxrd_batch(structures, n_workers=n_workers)\n",
    "    \n",
    "    # è®¡ç®—RWPå€¼\n",
    "    rwp_values = []\n",
    "    for calc_pxrd, obs_pxrd in zip(calculated_pxrds, observed_pxrds):\n",
    "        try:\n",
    "            from src.metrics import rwp\n",
    "            rwp_value = rwp(calc_pxrd, obs_pxrd)\n",
    "        except ImportError:\n",
    "            diff = calc_pxrd - obs_pxrd\n",
    "            weighted_diff = diff * np.sqrt(np.maximum(obs_pxrd, 1e-10))\n",
    "            rwp_value = np.sqrt(np.sum(weighted_diff**2) / np.sum(obs_pxrd**2 + 1e-10))\n",
    "        rwp_values.append(rwp_value)\n",
    "    \n",
    "    return rwp_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. åå¤„ç†å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_optimization(structure):\n",
    "    \"\"\"\n",
    "    èƒ½é‡ä¼˜åŒ–\n",
    "    \n",
    "    Args:\n",
    "        structure: å¾…ä¼˜åŒ–çš„Structureå¯¹è±¡\n",
    "    \n",
    "    Returns:\n",
    "        Structure: ä¼˜åŒ–åçš„ç»“æ„\n",
    "    \"\"\"\n",
    "    # TODO: å®ç°èƒ½é‡ä¼˜åŒ–\n",
    "    # å¯ä»¥ä½¿ç”¨GULPã€VASPã€æˆ–æœºå™¨å­¦ä¹ åŠ¿å‡½æ•°ç­‰\n",
    "    \n",
    "    # å ä½ï¼šç¨å¾®è°ƒæ•´æ™¶æ ¼å‚æ•°æ¨¡æ‹Ÿä¼˜åŒ–\n",
    "    new_lattice = structure.lattice.matrix * np.random.uniform(0.98, 1.02)\n",
    "    optimized = Structure(\n",
    "        lattice=Lattice(new_lattice),\n",
    "        species=structure.species,\n",
    "        coords=structure.frac_coords,\n",
    "        coords_are_cartesian=False\n",
    "    )\n",
    "    \n",
    "    return optimized\n",
    "\n",
    "def rietveld_refinement(structure, observed_pxrd):\n",
    "    \"\"\"\n",
    "    Rietveldç²¾ä¿®\n",
    "    \n",
    "    Args:\n",
    "        structure: å¾…ç²¾ä¿®çš„Structureå¯¹è±¡\n",
    "        observed_pxrd: è§‚æµ‹çš„PXRDè°±\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (ç²¾ä¿®åçš„Structure, æ˜¯å¦éœ€è¦ç²¾ä¿®)\n",
    "    \"\"\"\n",
    "    # åˆ¤æ–­æ˜¯å¦éœ€è¦ç²¾ä¿®\n",
    "    current_rwp = evaluate_structure_quality(structure, observed_pxrd)\n",
    "    needs_refinement = current_rwp > RWP_THRESHOLD * 1.5  # å¦‚æœRWPè¾ƒé«˜åˆ™éœ€è¦ç²¾ä¿®\n",
    "    \n",
    "    if not needs_refinement:\n",
    "        return structure, False\n",
    "    \n",
    "    # TODO: å®ç°Rietveldç²¾ä¿®\n",
    "    # å¯ä»¥ä½¿ç”¨GSAS-IIã€TOPASã€FullProfç­‰\n",
    "    \n",
    "    # å ä½ï¼šç¨å¾®è°ƒæ•´åŸå­ä½ç½®æ¨¡æ‹Ÿç²¾ä¿®\n",
    "    new_coords = structure.frac_coords + np.random.randn(*structure.frac_coords.shape) * 0.01\n",
    "    new_coords = np.clip(new_coords, 0, 1)  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…\n",
    "    \n",
    "    refined = Structure(\n",
    "        lattice=structure.lattice,\n",
    "        species=structure.species,\n",
    "        coords=new_coords,\n",
    "        coords_are_cartesian=False\n",
    "    )\n",
    "    \n",
    "    return refined, True\n",
    "\n",
    "def post_process_structure(structure, observed_pxrd):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„åå¤„ç†æµç¨‹\n",
    "    \n",
    "    Args:\n",
    "        structure: å¾…å¤„ç†çš„Structureå¯¹è±¡\n",
    "        observed_pxrd: è§‚æµ‹çš„PXRDè°±\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (å¤„ç†åçš„Structure, æœ€ç»ˆRWPå€¼)\n",
    "    \"\"\"\n",
    "    # 1. èƒ½é‡ä¼˜åŒ–\n",
    "    optimized = energy_optimization(structure)\n",
    "    rwp_after_opt = evaluate_structure_quality(optimized, observed_pxrd)\n",
    "    \n",
    "    # 2. Rietveldç²¾ä¿®ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "    refined, was_refined = rietveld_refinement(optimized, observed_pxrd)\n",
    "    \n",
    "    if was_refined:\n",
    "        rwp_after_refine = evaluate_structure_quality(refined, observed_pxrd)\n",
    "        return refined, rwp_after_refine\n",
    "    else:\n",
    "        return optimized, rwp_after_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ç»ˆæ­¢æ¡ä»¶æ£€æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_termination_conditions(sample_status):\n",
    "    \"\"\"\n",
    "    æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç»ˆæ­¢æ¡ä»¶\n",
    "    \n",
    "    ç»ˆæ­¢æ¡ä»¶ï¼š\n",
    "    1. è¿è¡Œæ—¶é—´è¶…è¿‡5å°æ—¶\n",
    "    2. æ‰€æœ‰æ ·æœ¬éƒ½æ»¡è¶³è´¨é‡è¦æ±‚æˆ–è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (æ˜¯å¦ç»ˆæ­¢, ç»ˆæ­¢åŸå› )\n",
    "    \"\"\"\n",
    "    # æ£€æŸ¥è¿è¡Œæ—¶é—´\n",
    "    elapsed_time = time.time() - START_TIME\n",
    "    if elapsed_time > MAX_RUNTIME:\n",
    "        return True, f\"è¾¾åˆ°æœ€å¤§è¿è¡Œæ—¶é—´ {MAX_TIME_HOURS} å°æ—¶\"\n",
    "    \n",
    "    # æ£€æŸ¥æ‰€æœ‰æ ·æœ¬çŠ¶æ€\n",
    "    all_done = all(\n",
    "        status['satisfied'] or status['attempts'] >= MAX_ATTEMPTS_PER_SAMPLE\n",
    "        for status in sample_status.values()\n",
    "    )\n",
    "    \n",
    "    if all_done:\n",
    "        satisfied_count = sum(1 for s in sample_status.values() if s['satisfied'])\n",
    "        return True, f\"æ‰€æœ‰æ ·æœ¬å¤„ç†å®Œæˆï¼ˆ{satisfied_count}/{len(sample_status)}æ»¡è¶³è¦æ±‚ï¼‰\"\n",
    "    \n",
    "    return False, None\n",
    "\n",
    "def get_samples_to_regenerate(sample_status, batch_size=32):\n",
    "    \"\"\"\n",
    "    è·å–éœ€è¦é‡æ–°ç”Ÿæˆçš„æ ·æœ¬\n",
    "    \n",
    "    Args:\n",
    "        sample_status: æ ·æœ¬çŠ¶æ€å­—å…¸\n",
    "        batch_size: æ‰¹æ¬¡å¤§å°\n",
    "    \n",
    "    Returns:\n",
    "        list: éœ€è¦é‡æ–°ç”Ÿæˆçš„æ ·æœ¬IDåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # æ‰¾å‡ºæœªæ»¡è¶³è¦æ±‚ä¸”æœªè¶…è¿‡å°è¯•æ¬¡æ•°çš„æ ·æœ¬\n",
    "    candidates = [\n",
    "        sample_id for sample_id, status in sample_status.items()\n",
    "        if not status['satisfied'] and status['attempts'] < MAX_ATTEMPTS_PER_SAMPLE\n",
    "    ]\n",
    "    \n",
    "    # æŒ‰RWPå€¼æ’åºï¼Œä¼˜å…ˆå¤„ç†è´¨é‡æœ€å·®çš„\n",
    "    candidates.sort(key=lambda x: sample_status[x]['best_rwp'], reverse=True)\n",
    "    \n",
    "    return candidates[:batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. åˆå§‹æ¨ç†ï¼ˆå¸¦å®æ—¶ä¿å­˜ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "é˜¶æ®µ1ï¼šåˆå§‹æ¨ç†ï¼ˆæ‰¹é‡å¤„ç†ï¼‰\n",
      "============================================================\n",
      "æ‰¹å¤„ç†é…ç½®:\n",
      "  æ¨ç†æ‰¹å¤§å°: 32\n",
      "  PXRDå¹¶è¡Œè¿›ç¨‹: 8\n",
      "\n",
      "ä½¿ç”¨æ¨¡å‹æ‰¹é‡ç”Ÿæˆ 200 ä¸ªç»“æ„...\n",
      "æ‰¹é‡è®¡ç®—PXRDå’Œè¯„ä¼°è´¨é‡...\n",
      "  å¤„ç†è¿›åº¦: 50/200\n",
      "  å¤„ç†è¿›åº¦: 100/200\n",
      "  å¤„ç†è¿›åº¦: 150/200\n",
      "  å¤„ç†è¿›åº¦: 200/200\n",
      "\n",
      "åˆå§‹æ¨ç†ç»“æœ:\n",
      "  æ€»æ ·æœ¬æ•°: 200\n",
      "  æ»¡è¶³è¦æ±‚: 0/200 (0.0%)\n",
      "  å¹³å‡RWP: 260.9022\n",
      "  RWPé˜ˆå€¼: 0.15\n",
      "\n",
      "RWPåˆ†å¸ƒ:\n",
      "  æœ€å°å€¼: 76.6063\n",
      "  25%åˆ†ä½: 166.7105\n",
      "  ä¸­ä½æ•°: 240.5362\n",
      "  75%åˆ†ä½: 324.7301\n",
      "  æœ€å¤§å€¼: 662.8995\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'update_submission_incrementally' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  æœ€å¤§å€¼: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.max(rwp_values)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# ç«‹å³ä¿å­˜åˆå§‹æ¨ç†ç»“æœåˆ°submission.csv\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m submission_df = \u001b[43mupdate_submission_incrementally\u001b[49m(sample_status, DATA_DIR, SUBMISSION_FILE)\n\u001b[32m     82\u001b[39m log_submission_update(\u001b[32m0\u001b[39m, sample_status, SUBMISSION_FILE)\n",
      "\u001b[31mNameError\u001b[39m: name 'update_submission_incrementally' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"é˜¶æ®µ1ï¼šåˆå§‹æ¨ç†ï¼ˆæ‰¹é‡å¤„ç†ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# é…ç½®æ‰¹å¤„ç†å‚æ•°\n",
    "INFERENCE_BATCH_SIZE = 32  # GPUæ¨ç†æ‰¹å¤§å°\n",
    "PXRD_WORKERS = min(mp.cpu_count() // 2, 8)  # PXRDè®¡ç®—å¹¶è¡Œè¿›ç¨‹æ•°\n",
    "\n",
    "print(f\"æ‰¹å¤„ç†é…ç½®:\")\n",
    "print(f\"  æ¨ç†æ‰¹å¤§å°: {INFERENCE_BATCH_SIZE}\")\n",
    "print(f\"  PXRDå¹¶è¡Œè¿›ç¨‹: {PXRD_WORKERS}\")\n",
    "\n",
    "initial_predictions = {}\n",
    "\n",
    "if model is not None:\n",
    "    # ä½¿ç”¨æ¨¡å‹æ‰¹é‡ç”Ÿæˆ\n",
    "    print(f\"\\nä½¿ç”¨æ¨¡å‹æ‰¹é‡ç”Ÿæˆ {len(df)} ä¸ªç»“æ„...\")\n",
    "    \n",
    "    # æ‰¹é‡ç”Ÿæˆæ‰€æœ‰ç»“æ„\n",
    "    all_structures = generate_crystal_structures_batch(\n",
    "        df, \n",
    "        model, \n",
    "        data_normalizer, \n",
    "        batch_size=INFERENCE_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # æ‰¹é‡è¯„ä¼°è´¨é‡\n",
    "    print(\"æ‰¹é‡è®¡ç®—PXRDå’Œè¯„ä¼°è´¨é‡...\")\n",
    "    all_observed_pxrds = df['pxrd'].tolist()\n",
    "    all_rwp_values = evaluate_structures_batch(\n",
    "        all_structures,\n",
    "        all_observed_pxrds,\n",
    "        n_workers=PXRD_WORKERS\n",
    "    )\n",
    "    \n",
    "    # æ›´æ–°æ ·æœ¬çŠ¶æ€\n",
    "    for idx, (sample_id, structure, rwp_value) in enumerate(zip(df['id'], all_structures, all_rwp_values)):\n",
    "        sample_status[sample_id]['attempts'] = 1\n",
    "        sample_status[sample_id]['best_rwp'] = rwp_value\n",
    "        sample_status[sample_id]['best_structure'] = structure\n",
    "        sample_status[sample_id]['satisfied'] = rwp_value < RWP_THRESHOLD\n",
    "        initial_predictions[sample_id] = structure\n",
    "        \n",
    "        # æ¯å¤„ç†50ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"  å¤„ç†è¿›åº¦: {idx + 1}/{len(df)}\")\n",
    "else:\n",
    "    # å¤‡ç”¨ï¼šé€ä¸ªç”Ÿæˆéšæœºç»“æ„\n",
    "    print(\"âš ï¸ ä½¿ç”¨éšæœºç”Ÿæˆï¼ˆæ¨¡å‹æœªåŠ è½½ï¼‰...\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"ç”Ÿæˆéšæœºç»“æ„\"):\n",
    "        sample_id = row['id']\n",
    "        structure = generate_random_structure(row)\n",
    "        rwp_value = evaluate_structure_quality(structure, row['pxrd'], pxrd_simulator)\n",
    "        \n",
    "        sample_status[sample_id]['attempts'] = 1\n",
    "        sample_status[sample_id]['best_rwp'] = rwp_value\n",
    "        sample_status[sample_id]['best_structure'] = structure\n",
    "        sample_status[sample_id]['satisfied'] = rwp_value < RWP_THRESHOLD\n",
    "        initial_predictions[sample_id] = structure\n",
    "\n",
    "# ç»Ÿè®¡åˆå§‹ç»“æœ\n",
    "satisfied_count = sum(1 for s in sample_status.values() if s['satisfied'])\n",
    "avg_rwp = np.mean([s['best_rwp'] for s in sample_status.values()])\n",
    "\n",
    "print(f\"\\nåˆå§‹æ¨ç†ç»“æœ:\")\n",
    "print(f\"  æ€»æ ·æœ¬æ•°: {len(sample_status)}\")\n",
    "print(f\"  æ»¡è¶³è¦æ±‚: {satisfied_count}/{len(sample_status)} ({satisfied_count/len(sample_status)*100:.1f}%)\")\n",
    "print(f\"  å¹³å‡RWP: {avg_rwp:.4f}\")\n",
    "print(f\"  RWPé˜ˆå€¼: {RWP_THRESHOLD}\")\n",
    "\n",
    "# æ˜¾ç¤ºRWPåˆ†å¸ƒ\n",
    "rwp_values = [s['best_rwp'] for s in sample_status.values()]\n",
    "print(f\"\\nRWPåˆ†å¸ƒ:\")\n",
    "print(f\"  æœ€å°å€¼: {np.min(rwp_values):.4f}\")\n",
    "print(f\"  25%åˆ†ä½: {np.percentile(rwp_values, 25):.4f}\")\n",
    "print(f\"  ä¸­ä½æ•°: {np.median(rwp_values):.4f}\")\n",
    "print(f\"  75%åˆ†ä½: {np.percentile(rwp_values, 75):.4f}\")\n",
    "print(f\"  æœ€å¤§å€¼: {np.max(rwp_values):.4f}\")\n",
    "\n",
    "# ç«‹å³ä¿å­˜åˆå§‹æ¨ç†ç»“æœåˆ°submission.csv\n",
    "submission_df = update_submission_incrementally(sample_status, DATA_DIR, SUBMISSION_FILE)\n",
    "log_submission_update(0, sample_status, SUBMISSION_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. è¿­ä»£ä¼˜åŒ–å¾ªç¯ï¼ˆå¸¦å®æ—¶ä¿å­˜ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"é˜¶æ®µ2ï¼šè¿­ä»£ä¼˜åŒ–ï¼ˆæ‰¹é‡å¤„ç† + CFGåŠ¨æ€è°ƒèŠ‚ï¼‰\")\nprint(\"=\"*60)\n\niteration = 0\nwhile True:\n    iteration += 1\n    \n    # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶\n    should_terminate, reason = check_termination_conditions(sample_status)\n    if should_terminate:\n        print(f\"\\nç»ˆæ­¢ä¼˜åŒ–: {reason}\")\n        break\n    \n    print(f\"\\n--- è¿­ä»£ {iteration} ---\")\n    elapsed = time.time() - START_TIME\n    print(f\"å·²è¿è¡Œ: {elapsed/3600:.2f}å°æ—¶\")\n    \n    # è·å–éœ€è¦ä¼˜åŒ–çš„æ ·æœ¬\n    samples_to_process = get_samples_to_regenerate(sample_status, BATCH_SIZE)\n    \n    if not samples_to_process:\n        print(\"æ²¡æœ‰éœ€è¦å¤„ç†çš„æ ·æœ¬\")\n        break\n    \n    print(f\"å¤„ç† {len(samples_to_process)} ä¸ªæ ·æœ¬\")\n    \n    # å‡†å¤‡æ‰¹å¤„ç†æ•°æ®\n    batch_df = df[df['id'].isin(samples_to_process)]\n    \n    # æ ‡è®°æ˜¯å¦æœ‰æ”¹è¿›\n    has_improvement = False\n    \n    # ========== CFGç­–ç•¥ï¼šæ ¹æ®è¿­ä»£æ¬¡æ•°åŠ¨æ€è°ƒæ•´å¼•å¯¼å¼ºåº¦ ==========\n    # æ—©æœŸè¿­ä»£ï¼šä½¿ç”¨æ ‡å‡†å¼•å¯¼å¼ºåº¦æ¢ç´¢\n    # ä¸­æœŸè¿­ä»£ï¼šå¢å¼ºå¼•å¯¼å¼ºåº¦æé«˜ç²¾åº¦\n    # åæœŸè¿­ä»£ï¼šé™ä½å¼•å¯¼å¼ºåº¦å¢åŠ å¤šæ ·æ€§\n    \n    if iteration <= 2:\n        # æ—©æœŸï¼šæ ‡å‡†å¼•å¯¼\n        current_cfg_scale = CFG_GUIDANCE_SCALE\n        print(f\"  CFGç­–ç•¥ï¼šæ—©æœŸæ¢ç´¢ï¼Œå¼•å¯¼å¼ºåº¦={current_cfg_scale:.2f}\")\n    elif iteration <= 5:\n        # ä¸­æœŸï¼šå¢å¼ºå¼•å¯¼\n        current_cfg_scale = min(CFG_GUIDANCE_SCALE * 1.5, CFG_MAX_SCALE)\n        print(f\"  CFGç­–ç•¥ï¼šç²¾ç¡®åŒ¹é…ï¼Œå¼•å¯¼å¼ºåº¦={current_cfg_scale:.2f}\")\n    else:\n        # åæœŸï¼šé™ä½å¼•å¯¼å¢åŠ å¤šæ ·æ€§\n        current_cfg_scale = max(CFG_GUIDANCE_SCALE * 0.8, CFG_MIN_SCALE)\n        print(f\"  CFGç­–ç•¥ï¼šå¢åŠ å¤šæ ·æ€§ï¼Œå¼•å¯¼å¼ºåº¦={current_cfg_scale:.2f}\")\n    \n    # å¯¹äºå¤šæ¬¡å¤±è´¥çš„æ ·æœ¬ï¼Œä½¿ç”¨æ›´æ¿€è¿›çš„å¼•å¯¼ç­–ç•¥\n    difficult_samples = [sid for sid in samples_to_process \n                        if sample_status[sid]['attempts'] >= 5]\n    \n    if difficult_samples:\n        print(f\"  å‘ç° {len(difficult_samples)} ä¸ªå›°éš¾æ ·æœ¬ï¼Œä½¿ç”¨è‡ªé€‚åº”CFG\")\n    \n    # ç­–ç•¥1ï¼šæ‰¹é‡åå¤„ç†å½“å‰æœ€ä½³ç»“æ„ï¼ˆç®€åŒ–ç‰ˆï¼‰\n    # æ³¨ï¼šå®é™…çš„èƒ½é‡ä¼˜åŒ–å’ŒRietveldç²¾ä¿®éœ€è¦ä¸“é—¨çš„æ‰¹é‡å®ç°\n    \n    # ç­–ç•¥2ï¼šæ‰¹é‡é‡æ–°ç”Ÿæˆï¼ˆä½¿ç”¨åŠ¨æ€CFGï¼‰\n    if model is not None:\n        print(f\"  æ‰¹é‡é‡æ–°ç”Ÿæˆ {len(batch_df)} ä¸ªç»“æ„...\")\n        \n        # ä¸ºå›°éš¾æ ·æœ¬ä½¿ç”¨ä¸åŒçš„å¼•å¯¼å¼ºåº¦\n        if difficult_samples:\n            # åˆ†ä¸¤æ‰¹å¤„ç†ï¼šå›°éš¾æ ·æœ¬å’Œæ™®é€šæ ·æœ¬\n            difficult_df = batch_df[batch_df['id'].isin(difficult_samples)]\n            normal_df = batch_df[~batch_df['id'].isin(difficult_samples)]\n            \n            new_structures = []\n            scales_used = []\n            \n            # å›°éš¾æ ·æœ¬ï¼šä½¿ç”¨è‡ªé€‚åº”CFG\n            if len(difficult_df) > 0:\n                print(f\"    å¤„ç†å›°éš¾æ ·æœ¬ï¼ˆè‡ªé€‚åº”CFGï¼‰...\")\n                diff_structures, diff_scales = generate_crystal_structures_batch_cfg(\n                    difficult_df,\n                    model,\n                    data_normalizer,\n                    batch_size=INFERENCE_BATCH_SIZE,\n                    guidance_scale=None,  # ä½¿ç”¨è‡ªé€‚åº”\n                    adaptive_mode=True\n                )\n                new_structures.extend(diff_structures)\n                scales_used.extend(diff_scales)\n                \n                # æ˜¾ç¤ºä½¿ç”¨çš„å¼•å¯¼å¼ºåº¦åˆ†å¸ƒ\n                print(f\"      å¼•å¯¼å¼ºåº¦èŒƒå›´: [{min(diff_scales):.2f}, {max(diff_scales):.2f}]\")\n            \n            # æ™®é€šæ ·æœ¬ï¼šä½¿ç”¨å½“å‰è¿­ä»£çš„å¼•å¯¼å¼ºåº¦\n            if len(normal_df) > 0:\n                print(f\"    å¤„ç†æ™®é€šæ ·æœ¬ï¼ˆCFG={current_cfg_scale:.2f}ï¼‰...\")\n                norm_structures, norm_scales = generate_crystal_structures_batch_cfg(\n                    normal_df,\n                    model,\n                    data_normalizer,\n                    batch_size=INFERENCE_BATCH_SIZE,\n                    guidance_scale=current_cfg_scale,\n                    adaptive_mode=False\n                )\n                new_structures.extend(norm_structures)\n                scales_used.extend(norm_scales)\n            \n            # é‡æ–°æ’åºä»¥åŒ¹é…åŸå§‹batch_dfé¡ºåº\n            id_to_structure = dict(zip(\n                list(difficult_df['id']) + list(normal_df['id']),\n                new_structures\n            ))\n            id_to_scale = dict(zip(\n                list(difficult_df['id']) + list(normal_df['id']),\n                scales_used\n            ))\n            \n            new_structures = [id_to_structure[sid] for sid in batch_df['id']]\n            scales_used = [id_to_scale[sid] for sid in batch_df['id']]\n            \n        else:\n            # æ‰€æœ‰æ ·æœ¬ä½¿ç”¨ç›¸åŒçš„å¼•å¯¼å¼ºåº¦\n            new_structures, scales_used = generate_crystal_structures_batch_cfg(\n                batch_df,\n                model,\n                data_normalizer,\n                batch_size=INFERENCE_BATCH_SIZE,\n                guidance_scale=current_cfg_scale,\n                adaptive_mode=False\n            )\n        \n        # æ‰¹é‡è¯„ä¼°æ–°ç»“æ„\n        print(\"  æ‰¹é‡è¯„ä¼°æ–°ç»“æ„è´¨é‡...\")\n        batch_observed_pxrds = batch_df['pxrd'].tolist()\n        new_rwp_values = evaluate_structures_batch(\n            new_structures,\n            batch_observed_pxrds,\n            n_workers=PXRD_WORKERS\n        )\n        \n        # æ›´æ–°çŠ¶æ€ï¼ˆä¿ç•™æœ€ä½³ç»“æœï¼‰\n        improvements = []\n        for sample_id, new_structure, new_rwp, used_scale in zip(\n            batch_df['id'], new_structures, new_rwp_values, scales_used\n        ):\n            current_best_rwp = sample_status[sample_id]['best_rwp']\n            \n            # å¦‚æœæ–°ç»“æ„æ›´å¥½ï¼Œåˆ™æ›´æ–°\n            if new_rwp < current_best_rwp:\n                improvement_ratio = (current_best_rwp - new_rwp) / current_best_rwp\n                improvements.append((sample_id, improvement_ratio, used_scale))\n                \n                sample_status[sample_id]['best_structure'] = new_structure\n                sample_status[sample_id]['best_rwp'] = new_rwp\n                sample_status[sample_id]['satisfied'] = new_rwp < RWP_THRESHOLD\n                has_improvement = True\n            \n            # æ›´æ–°å°è¯•æ¬¡æ•°\n            sample_status[sample_id]['attempts'] += 1\n        \n        # æ˜¾ç¤ºæ”¹è¿›æœ€å¤§çš„æ ·æœ¬\n        if improvements:\n            improvements.sort(key=lambda x: x[1], reverse=True)\n            print(f\"\\n  æœ€ä½³æ”¹è¿›æ ·æœ¬:\")\n            for sid, ratio, scale in improvements[:3]:  # æ˜¾ç¤ºå‰3ä¸ª\n                print(f\"    {sid}: æ”¹è¿›{ratio*100:.1f}% (CFG={scale:.2f})\")\n    \n    else:\n        # å¤‡ç”¨ï¼šé€ä¸ªå¤„ç†ï¼ˆä½¿ç”¨éšæœºç”Ÿæˆï¼‰\n        for sample_id in tqdm(samples_to_process, desc=f\"è¿­ä»£{iteration}\"):\n            row = df[df['id'] == sample_id].iloc[0]\n            new_structure = generate_random_structure(row)\n            new_rwp = evaluate_structure_quality(new_structure, row['pxrd'], pxrd_simulator)\n            \n            current_best_rwp = sample_status[sample_id]['best_rwp']\n            if new_rwp < current_best_rwp:\n                sample_status[sample_id]['best_structure'] = new_structure\n                sample_status[sample_id]['best_rwp'] = new_rwp\n                sample_status[sample_id]['satisfied'] = new_rwp < RWP_THRESHOLD\n                has_improvement = True\n            \n            sample_status[sample_id]['attempts'] += 1\n    \n    # ç»Ÿè®¡å½“å‰çŠ¶æ€\n    satisfied_count = sum(1 for s in sample_status.values() if s['satisfied'])\n    avg_rwp = np.mean([s['best_rwp'] for s in sample_status.values()])\n    \n    print(f\"\\nè¿­ä»£{iteration}ç»“æœ:\")\n    print(f\"  æ»¡è¶³è¦æ±‚: {satisfied_count}/{len(sample_status)} ({satisfied_count/len(sample_status)*100:.1f}%)\")\n    print(f\"  å¹³å‡RWP: {avg_rwp:.4f}\")\n    \n    if has_improvement:\n        print(\"  âœ¨ æœ¬è½®æœ‰æ ·æœ¬å¾—åˆ°æ”¹è¿›\")\n        \n        # æ˜¾ç¤ºæ”¹è¿›çš„æ ·æœ¬æ•°\n        improved_count = sum(1 for sid in samples_to_process \n                            if sample_status[sid]['attempts'] > 1 \n                            and sample_status[sid]['satisfied'])\n        if improved_count > 0:\n            print(f\"  ğŸ“ˆ æ–°æ»¡è¶³è¦æ±‚çš„æ ·æœ¬: {improved_count}\")\n    \n    # æ¯æ¬¡è¿­ä»£åéƒ½æ›´æ–°submission.csv\n    submission_df = update_submission_incrementally(sample_status, DATA_DIR, SUBMISSION_FILE)\n    log_submission_update(iteration, sample_status, SUBMISSION_FILE)\n    \n    # é™åˆ¶è¿­ä»£æ¬¡æ•°ï¼ˆé¢å¤–ä¿æŠ¤ï¼‰\n    if iteration > 100:\n        print(\"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°\")\n        break\n    \n    # æå‰ç»ˆæ­¢ï¼šå¦‚æœæ²¡æœ‰æ”¹è¿›ä¸”å°è¯•æ¬¡æ•°è¾ƒå¤š\n    if not has_improvement and iteration > 5:\n        print(\"è¿ç»­å¤šè½®æ— æ”¹è¿›ï¼Œæå‰ç»ˆæ­¢\")\n        break\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"è¿­ä»£ä¼˜åŒ–å®Œæˆ\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. å¢é‡æ›´æ–°submission.csvå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_submission_incrementally(sample_status, data_dir, output_file=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    å¢é‡æ›´æ–°submission.csvæ–‡ä»¶\n",
    "    æ¯æ¬¡è°ƒç”¨æ—¶é‡æ–°ç”Ÿæˆæ•´ä¸ªæ–‡ä»¶ï¼Œç¡®ä¿åŒ…å«æœ€æ–°çš„æ‰€æœ‰ç»“æœ\n",
    "    \n",
    "    Args:\n",
    "        sample_status: æ ·æœ¬çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«æ¯ä¸ªæ ·æœ¬çš„æœ€ä½³ç»“æ„\n",
    "        data_dir: æ•°æ®ç›®å½•è·¯å¾„\n",
    "        output_file: è¾“å‡ºæ–‡ä»¶å\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: submissionæ•°æ®æ¡†\n",
    "    \"\"\"\n",
    "    # è·å–IDå‰ç¼€ï¼ˆAæˆ–Bï¼‰\n",
    "    with open(data_dir / \"composition.json\", 'r') as f:\n",
    "        composition_dict = json.load(f)\n",
    "    prefix = next(iter(composition_dict))[0]  # è·å–ç¬¬ä¸€ä¸ªIDçš„é¦–å­—æ¯\n",
    "    \n",
    "    # å‡†å¤‡submissionæ•°æ®\n",
    "    rows = []\n",
    "    \n",
    "    for sample_id, status in sample_status.items():\n",
    "        try:\n",
    "            structure = status['best_structure']\n",
    "            \n",
    "            if structure is not None:\n",
    "                # è½¬æ¢ä¸ºCIFæ ¼å¼\n",
    "                cif_str = structure.to(fmt=\"cif\")\n",
    "            else:\n",
    "                # å¦‚æœè¿˜æ²¡æœ‰ç»“æ„ï¼Œåˆ›å»ºå ä½CIF\n",
    "                cif_str = f\"data_{sample_id}\\n_cell_length_a 5.0\\n_cell_length_b 5.0\\n_cell_length_c 5.0\\n_cell_angle_alpha 90\\n_cell_angle_beta 90\\n_cell_angle_gamma 90\\n\"\n",
    "            \n",
    "            rows.append({\n",
    "                'ID': sample_id,\n",
    "                'cif': cif_str\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # å‡ºé”™æ—¶åˆ›å»ºå ä½CIF\n",
    "            min_cif = f\"data_{sample_id}\\n_cell_length_a 5.0\\n_cell_length_b 5.0\\n_cell_length_c 5.0\\n_cell_angle_alpha 90\\n_cell_angle_beta 90\\n_cell_angle_gamma 90\\n\"\n",
    "            rows.append({\n",
    "                'ID': sample_id,\n",
    "                'cif': min_cif\n",
    "            })\n",
    "    \n",
    "    # åˆ›å»ºDataFrame\n",
    "    submission_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # ä¿å­˜ä¸ºCSVï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "def log_submission_update(iteration, sample_status, submission_file=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    è®°å½•submissionæ›´æ–°ä¿¡æ¯\n",
    "    \n",
    "    Args:\n",
    "        iteration: å½“å‰è¿­ä»£è½®æ¬¡ï¼ˆ0è¡¨ç¤ºåˆå§‹æ¨ç†ï¼‰\n",
    "        sample_status: æ ·æœ¬çŠ¶æ€å­—å…¸\n",
    "        submission_file: submissionæ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    satisfied_count = sum(1 for s in sample_status.values() if s['satisfied'])\n",
    "    total_count = len(sample_status)\n",
    "    \n",
    "    if iteration == 0:\n",
    "        print(f\"\\nğŸ“ åˆå§‹submission.csvå·²ç”Ÿæˆ\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“ submission.csvå·²æ›´æ–° (è¿­ä»£{iteration})\")\n",
    "    \n",
    "    print(f\"   æ»¡è¶³è¦æ±‚: {satisfied_count}/{total_count} ({satisfied_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    if os.path.exists(submission_file):\n",
    "        file_size = os.path.getsize(submission_file) / 1024\n",
    "        print(f\"   æ–‡ä»¶å¤§å°: {file_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. æœ€ç»ˆç»Ÿè®¡å’ŒéªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆç»Ÿè®¡\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æœ€ç»ˆç»Ÿè®¡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è®¡ç®—å„é¡¹ç»Ÿè®¡æŒ‡æ ‡\n",
    "satisfied_samples = [s for s in sample_status.values() if s['satisfied']]\n",
    "unsatisfied_samples = [s for s in sample_status.values() if not s['satisfied']]\n",
    "\n",
    "print(f\"\\nè´¨é‡ç»Ÿè®¡:\")\n",
    "print(f\"  æ»¡è¶³RWP<{RWP_THRESHOLD}: {len(satisfied_samples)}/{len(sample_status)} ({len(satisfied_samples)/len(sample_status)*100:.1f}%)\")\n",
    "print(f\"  æœªæ»¡è¶³è¦æ±‚: {len(unsatisfied_samples)}\")\n",
    "\n",
    "if satisfied_samples:\n",
    "    satisfied_rwps = [s['best_rwp'] for s in satisfied_samples]\n",
    "    print(f\"\\næ»¡è¶³è¦æ±‚æ ·æœ¬çš„RWP:\")\n",
    "    print(f\"  æœ€å°: {np.min(satisfied_rwps):.4f}\")\n",
    "    print(f\"  æœ€å¤§: {np.max(satisfied_rwps):.4f}\")\n",
    "    print(f\"  å¹³å‡: {np.mean(satisfied_rwps):.4f}\")\n",
    "\n",
    "if unsatisfied_samples:\n",
    "    unsatisfied_rwps = [s['best_rwp'] for s in unsatisfied_samples]\n",
    "    print(f\"\\næœªæ»¡è¶³è¦æ±‚æ ·æœ¬çš„RWP:\")\n",
    "    print(f\"  æœ€å°: {np.min(unsatisfied_rwps):.4f}\")\n",
    "    print(f\"  æœ€å¤§: {np.max(unsatisfied_rwps):.4f}\")\n",
    "    print(f\"  å¹³å‡: {np.mean(unsatisfied_rwps):.4f}\")\n",
    "\n",
    "# å°è¯•æ¬¡æ•°ç»Ÿè®¡\n",
    "attempts_list = [s['attempts'] for s in sample_status.values()]\n",
    "print(f\"\\nå°è¯•æ¬¡æ•°ç»Ÿè®¡:\")\n",
    "print(f\"  æœ€å°‘: {np.min(attempts_list)}\")\n",
    "print(f\"  æœ€å¤š: {np.max(attempts_list)}\")\n",
    "print(f\"  å¹³å‡: {np.mean(attempts_list):.1f}\")\n",
    "print(f\"  è¾¾åˆ°ä¸Šé™({MAX_ATTEMPTS_PER_SAMPLE}æ¬¡): {sum(1 for a in attempts_list if a >= MAX_ATTEMPTS_PER_SAMPLE)}\")\n",
    "\n",
    "# è¿è¡Œæ—¶é—´\n",
    "total_time = time.time() - START_TIME\n",
    "print(f\"\\næ€»è¿è¡Œæ—¶é—´: {total_time/3600:.2f}å°æ—¶\")\n",
    "\n",
    "# éªŒè¯æœ€ç»ˆçš„æäº¤æ–‡ä»¶\n",
    "print(f\"\\néªŒè¯æœ€ç»ˆsubmissionæ–‡ä»¶:\")\n",
    "if os.path.exists(SUBMISSION_FILE):\n",
    "    # é‡æ–°è¯»å–æ–‡ä»¶ä»¥éªŒè¯\n",
    "    final_submission = pd.read_csv(SUBMISSION_FILE)\n",
    "    print(f\"  æ–‡ä»¶å: {SUBMISSION_FILE}\")\n",
    "    print(f\"  æ–‡ä»¶å¤§å°: {os.path.getsize(SUBMISSION_FILE) / 1024:.2f} KB\")\n",
    "    print(f\"  æ ·æœ¬æ•°: {len(final_submission)}\")\n",
    "    print(f\"  åˆ—å: {final_submission.columns.tolist()}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼\n",
    "    missing = final_submission.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(f\"\\nâš ï¸ è­¦å‘Šï¼šå‘ç°ç¼ºå¤±å€¼ï¼\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(f\"  âœ… æ²¡æœ‰ç¼ºå¤±å€¼\")\n",
    "    \n",
    "    # æ£€æŸ¥IDæ˜¯å¦å®Œæ•´\n",
    "    expected_ids = set(sample_status.keys())\n",
    "    actual_ids = set(final_submission['ID'].values)\n",
    "    if expected_ids == actual_ids:\n",
    "        print(f\"  âœ… æ‰€æœ‰æ ·æœ¬IDéƒ½å·²åŒ…å«\")\n",
    "    else:\n",
    "        missing_ids = expected_ids - actual_ids\n",
    "        extra_ids = actual_ids - expected_ids\n",
    "        if missing_ids:\n",
    "            print(f\"  âš ï¸ ç¼ºå°‘ID: {missing_ids}\")\n",
    "        if extra_ids:\n",
    "            print(f\"  âš ï¸ å¤šä½™ID: {extra_ids}\")\n",
    "else:\n",
    "    print(f\"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {SUBMISSION_FILE}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… æ¨ç†å®Œæˆï¼submission.csvå·²åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­å®æ—¶æ›´æ–°\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "æœ¬notebookå®ç°äº†è¿­ä»£ä¼˜åŒ–çš„æ¨ç†æµç¨‹ï¼Œå¹¶**å®æ—¶æ›´æ–°submission.csv**ï¼š\n",
    "\n",
    "### æ ¸å¿ƒæµç¨‹\n",
    "1. âœ… **åˆå§‹æ¨ç†**ï¼šæ¨¡å‹æ¨ç†ç”Ÿæˆåˆå§‹ç»“æ„ï¼Œç«‹å³ä¿å­˜submission.csv\n",
    "2. âœ… **è´¨é‡è¯„ä¼°**ï¼šä½¿ç”¨RWPæŒ‡æ ‡è¯„ä¼°PXRDåŒ¹é…è´¨é‡\n",
    "3. âœ… **è¿­ä»£ä¼˜åŒ–**ï¼š\n",
    "   - èƒ½é‡ä¼˜åŒ– + Rietveldç²¾ä¿®\n",
    "   - **æ¯è½®è¿­ä»£åç«‹å³æ›´æ–°submission.csv**\n",
    "   - æ‰¹é‡é‡æ–°ç”Ÿæˆä¸æ»¡è¶³è¦æ±‚çš„æ ·æœ¬\n",
    "4. âœ… **ç»ˆæ­¢æ¡ä»¶**ï¼š\n",
    "   - è¿è¡Œæ—¶é—´é™åˆ¶ï¼ˆ5å°æ—¶ï¼‰\n",
    "   - å•æ ·æœ¬å°è¯•æ¬¡æ•°é™åˆ¶\n",
    "   - å…¨éƒ¨æ»¡è¶³è¦æ±‚\n",
    "\n",
    "### å…³é”®æ”¹è¿›\n",
    "- ğŸ”„ **å¢é‡æ›´æ–°æœºåˆ¶**ï¼šæ¯æ¬¡æ¨ç†/ä¼˜åŒ–åç«‹å³è¦†ç›–submission.csv\n",
    "- ğŸ“Š **å®æ—¶è¿›åº¦åé¦ˆ**ï¼šè¯„æµ‹è„šæœ¬å¯ä»¥éšæ—¶è¯»å–æœ€æ–°ç»“æœ\n",
    "- ğŸ›¡ï¸ **æ–­ç‚¹ç»­ä¼ æ”¯æŒ**ï¼šå³ä½¿ä¸­é€”ä¸­æ–­ï¼Œå·²æœ‰ç»“æœä¹Ÿä¿å­˜åœ¨submission.csvä¸­\n",
    "- ğŸ“ **æ›´æ–°æ—¥å¿—**ï¼šæ¯æ¬¡æ›´æ–°éƒ½è®°å½•çŠ¶æ€ä¿¡æ¯ï¼ˆæ»¡è¶³ç‡ã€æ–‡ä»¶å¤§å°ç­‰ï¼‰\n",
    "\n",
    "### å¾…å®ç°éƒ¨åˆ†\n",
    "- â³ å®é™…çš„æ¨¡å‹æ¨ç†\n",
    "- â³ PXRDè®¡ç®—ï¼ˆè°ƒç”¨PXRDSimulatorï¼‰\n",
    "- â³ èƒ½é‡ä¼˜åŒ–ï¼ˆGULPç­‰ï¼‰\n",
    "- â³ Rietveldç²¾ä¿®ï¼ˆGSAS-IIç­‰ï¼‰\n",
    "\n",
    "### è¾“å‡º\n",
    "- âœ… ç¬¦åˆæ¯”èµ›è¦æ±‚çš„submission.csvï¼ˆCIFæ ¼å¼ï¼‰\n",
    "- âœ… **å®æ—¶æ›´æ–°**ï¼šæ¯è½®æ¨ç†åç«‹å³ä¿å­˜ï¼Œè¯„æµ‹è„šæœ¬å¯åŠæ—¶è¯»å–\n",
    "- âœ… è¯¦ç»†çš„ä¼˜åŒ–è¿‡ç¨‹è®°å½•å’Œç»Ÿè®¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}