#!/usr/bin/env python
"""
æ™¶ä½“ç»“æ„ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬
æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’ŒDDPåŒå¡è®­ç»ƒ
"""

import argparse
import os
from pathlib import Path
from datetime import datetime
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.strategies import DDPStrategy
from pytorch_lightning.loggers import CSVLogger
import warnings
import json

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.trainer import CrystalGenerationModule, CrystalGenerationDataModule
from src.ema_callback import EMACallback  # å¯¼å…¥EMAå›è°ƒ

# å¿½ç•¥ä¸€äº›æ— å®³çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='torch.nn.parallel')


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Train Crystal Structure Generation Model')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--network', type=str, default='transformer', 
                        choices=['transformer', 'equiformer'],
                        help='Network architecture to use')
    parser.add_argument('--flow', type=str, default='cfm',
                        choices=['cfm', 'meanflow'],
                        help='Flow model to use')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--train_path', type=str, 
                        default='data/merged_full_mp_cdvae_train.pkl',
                        help='Path to training data')
    parser.add_argument('--val_path', type=str,
                        default='data/merged_full_mp_cdvae_val.pkl',
                        help='Path to validation data')
    parser.add_argument('--test_path', type=str,
                        default='data/A_sample.pkl',
                        help='Path to test data (optional)')
    parser.add_argument('--batch_size', type=int, default=32,  # å‡å°batch sizeæé«˜ç¨³å®šæ€§
                        help='Batch size per GPU')
    parser.add_argument('--num_workers', type=int, default=16,  # å¢åŠ åˆ°8ä¸ªï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸CPU
                        help='Number of data loading workers')
    parser.add_argument('--no_augment_permutation', action='store_true',
                        help='Disable permutation data augmentation for atoms')
    parser.add_argument('--no_augment_so3', action='store_true', 
                        help='Disable SO3 data augmentation for lattice rotation')
    parser.add_argument('--augment_prob', type=float, default=0.8,
                        help='Probability of applying permutation augmentation')
    parser.add_argument('--so3_augment_prob', type=float, default=0.8,
                        help='Probability of applying SO3 augmentation')
    
    # ç½‘ç»œå‚æ•°
    parser.add_argument('--hidden_dim', type=int, default=512,  # å‡å°æ¨¡å‹è§„æ¨¡é˜²æ­¢è¿‡æ‹Ÿåˆ
                        help='Hidden dimension for transformer')
    parser.add_argument('--num_layers', type=int, default=10,  # å‡å°‘å±‚æ•°
                        help='Number of transformer layers')
    parser.add_argument('--num_heads', type=int, default=8,
                        help='Number of attention heads')
    parser.add_argument('--dropout', type=float, default=0.1,  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
                        help='Dropout rate')
    
    # æµæ¨¡å‹å‚æ•°
    parser.add_argument('--sigma_min', type=float, default=1e-4,
                        help='Minimum noise level for CFM')
    parser.add_argument('--sigma_max', type=float, default=1.0,
                        help='Maximum noise level for CFM')
    parser.add_argument('--loss_weight_lattice', type=float, default=2.0,
                        help='Loss weight for lattice parameters')
    parser.add_argument('--loss_weight_coords', type=float, default=1.0,
                        help='Loss weight for fractional coordinates')
    parser.add_argument('--default_num_steps', type=int, default=50,
                        help='Default number of sampling steps')
    
    # ä¼˜åŒ–å™¨å‚æ•°
    parser.add_argument('--lr', type=float, default=5e-5,  # é™ä½å­¦ä¹ ç‡
                        help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=0.05,  # å¢åŠ æƒé‡è¡°å‡
                        help='Weight decay')
    parser.add_argument('--optimizer', type=str, default='AdamW',
                        choices=['AdamW', 'Adam'],
                        help='Optimizer type')
    
    # è°ƒåº¦å™¨å‚æ•°
    parser.add_argument('--scheduler', type=str, default='ReduceLROnPlateau',  # ä½¿ç”¨è‡ªé€‚åº”è°ƒåº¦å™¨
                        choices=['none', 'CosineAnnealingLR', 'CosineAnnealingWarmup', 
                                 'StepLR', 'ReduceLROnPlateau'],
                        help='Learning rate scheduler type')
    parser.add_argument('--warmup_steps', type=int, default=2000,  # å¢åŠ warmupæ­¥æ•°
                        help='Warmup steps for CosineAnnealingWarmup')
    parser.add_argument('--max_steps', type=int, default=100000,
                        help='Max steps for CosineAnnealingWarmup')
    parser.add_argument('--T_max', type=int, default=100,
                        help='T_max for CosineAnnealingLR')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--max_epochs', type=int, default=500,  # å‡å°‘epoché¿å…è¿‡æ‹Ÿåˆ
                        help='Maximum number of epochs')
    parser.add_argument('--gradient_clip_val', type=float, default=1.0,  # ä½¿ç”¨æ ‡å‡†æ¢¯åº¦è£å‰ª
                        help='Gradient clipping value (reduced for stability)')
    parser.add_argument('--accumulate_grad_batches', type=int, default=2,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
                        help='Accumulate gradients over k batches')
    parser.add_argument('--val_check_interval', type=float, default=0.5,  # æ›´é¢‘ç¹åœ°éªŒè¯
                        help='How often to check validation set')
    parser.add_argument('--log_every_n_steps', type=int, default=20,  # æ›´é¢‘ç¹åœ°è®°å½•
                        help='Log metrics every n steps')
    
    # EMAå‚æ•°
    parser.add_argument('--use_ema', action='store_true', default=True,
                        help='Use Exponential Moving Average')
    parser.add_argument('--ema_decay', type=float, default=0.9999,
                        help='EMA decay rate (closer to 1 = more smoothing)')
    
    # Checkpointå‚æ•°
    parser.add_argument('--output_dir', type=str, default='outputs',
                        help='Base directory for all outputs')
    parser.add_argument('--exp_name', type=str, default=None,
                        help='Experiment name (if not provided, auto-generated)')
    parser.add_argument('--save_top_k', type=int, default=3,
                        help='Save top k checkpoints')
    parser.add_argument('--patience', type=int, default=30,  # å¢åŠ è€å¿ƒå€¼
                        help='Early stopping patience')
    parser.add_argument('--resume_from', type=str, default=None,
                        help='Resume training from checkpoint')
    
    # ç¡¬ä»¶å‚æ•°
    parser.add_argument('--gpus', type=int, default=-1,
                        help='Number of GPUs to use (-1 for all available)')
    parser.add_argument('--precision', type=str, default='32',
                        choices=['32', '16-mixed', 'bf16-mixed'],
                        help='Training precision')
    parser.add_argument('--deterministic', action='store_true',
                        help='Use deterministic training')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed')
    
    # è°ƒè¯•å‚æ•°
    parser.add_argument('--fast_dev_run', action='store_true',
                        help='Fast development run (1 batch)')
    parser.add_argument('--overfit_batches', type=float, default=0,
                        help='Overfit on a fraction of batches')
    parser.add_argument('--profile', action='store_true',
                        help='Profile training performance')
    
    return parser.parse_args()


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(args.seed, workers=True)
    
    # åˆ›å»ºå®éªŒåç§°ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if args.exp_name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.exp_name = f"{args.network}_{args.flow}_{timestamp}"
    else:
        # å¦‚æœæä¾›äº†å®éªŒåç§°ï¼Œä¹Ÿæ·»åŠ æ—¶é—´æˆ³é¿å…è¦†ç›–
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.exp_name = f"{args.exp_name}_{timestamp}"
    
    # åˆ›å»ºå®éªŒè¾“å‡ºç›®å½•
    exp_dir = Path(args.output_dir) / args.exp_name
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•
    checkpoint_dir = exp_dir / "checkpoints"
    logs_dir = exp_dir / "logs"
    config_dir = exp_dir / "config"
    
    checkpoint_dir.mkdir(exist_ok=True)
    logs_dir.mkdir(exist_ok=True)
    config_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    config_path = config_dir / "train_config.json"
    with open(config_path, 'w') as f:
        json.dump(vars(args), f, indent=2, default=str)
    
    print(f"\nğŸ“ Experiment directory: {exp_dir}")
    print(f"  â”œâ”€â”€ ğŸ“¦ Checkpoints: {checkpoint_dir}")
    print(f"  â”œâ”€â”€ ğŸ“Š Logs: {logs_dir}")
    print(f"  â””â”€â”€ âš™ï¸  Config: {config_dir}")
    
    # å‡†å¤‡ç½‘ç»œé…ç½®
    network_config = {
        'hidden_dim': args.hidden_dim,
        'num_layers': args.num_layers,
        'num_heads': args.num_heads,
        'dropout': args.dropout,
        'max_atoms': 60,
        'pxrd_dim': 11501,
    }
    
    # å‡†å¤‡æµæ¨¡å‹é…ç½®
    flow_config = {
        'sigma_min': args.sigma_min,
        'sigma_max': args.sigma_max,
        'loss_weight_lattice': args.loss_weight_lattice,
        'loss_weight_coords': args.loss_weight_coords,
        'default_num_steps': args.default_num_steps,
        # TODO(human): æ·»åŠ å½’ä¸€åŒ–ç›¸å…³é…ç½®
    }
    
    # å‡†å¤‡ä¼˜åŒ–å™¨é…ç½®
    optimizer_config = {
        'type': args.optimizer,
        'lr': args.lr,
        'weight_decay': args.weight_decay,
        'betas': (0.9, 0.999),
    }
    
    # å‡†å¤‡è°ƒåº¦å™¨é…ç½®
    scheduler_config = None
    if args.scheduler != 'none':
        scheduler_config = {
            'type': args.scheduler,
            'warmup_steps': args.warmup_steps,
            'max_steps': args.max_steps,
            'T_max': args.T_max,
            'eta_min': 1e-6,
            'step_size': 30,
            'gamma': 0.1,
            'factor': 0.5,
            'patience': 10,
            'min_lr': 1e-6,
        }
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ“Š Creating model with {args.network} network and {args.flow} flow...")
    model = CrystalGenerationModule(
        network_name=args.network,
        flow_name=args.flow,
        network_config=network_config,
        flow_config=flow_config,
        optimizer_config=optimizer_config,
        scheduler_config=scheduler_config,
    )
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    print(f"ğŸ“ Loading data from {args.train_path}...")
    # é»˜è®¤å¯ç”¨æ•°æ®å¢å¼ºï¼Œé™¤éæ˜ç¡®ç¦ç”¨
    augment_permutation = not args.no_augment_permutation
    augment_so3 = not args.no_augment_so3
    
    print(f"  Permutation augmentation: {'Enabled' if augment_permutation else 'Disabled'}")
    print(f"  SO3 augmentation: {'Enabled' if augment_so3 else 'Disabled'}")
    
    data_module = CrystalGenerationDataModule(
        train_path=args.train_path,
        val_path=args.val_path,
        test_path=args.test_path,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        augment_permutation=augment_permutation,
        augment_so3=augment_so3,
        augment_prob=args.augment_prob,
        so3_augment_prob=args.so3_augment_prob
    )
    
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    csv_logger = CSVLogger(
        save_dir=logs_dir,
        name="metrics",
        version=""  # ä¸ä½¿ç”¨ç‰ˆæœ¬å·å­ç›®å½•
    )
    
    # è®¾ç½®å›è°ƒ
    callbacks = []
    
    # EMAå›è°ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.use_ema:
        ema_callback = EMACallback(decay=args.ema_decay, update_every=1)
        callbacks.append(ema_callback)
        print(f"âœ¨ EMA enabled with decay={args.ema_decay}")
    
    # Checkpointå›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename='epoch{epoch:03d}-val_loss{val/loss:.4f}',
        monitor='val/loss',
        mode='min',
        save_top_k=args.save_top_k,
        save_last=True,
        verbose=True,
    )
    callbacks.append(checkpoint_callback)
    
    # æ—©åœå›è°ƒ
    if args.patience > 0:
        early_stopping = EarlyStopping(
            monitor='val/loss',
            patience=args.patience,
            mode='min',
            verbose=True,
        )
        callbacks.append(early_stopping)
    
    # å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval='step')
    callbacks.append(lr_monitor)
    
    # å‡†å¤‡Trainerå‚æ•°
    trainer_kwargs = {
        'max_epochs': args.max_epochs,
        'gradient_clip_val': args.gradient_clip_val,
        'accumulate_grad_batches': args.accumulate_grad_batches,
        'val_check_interval': args.val_check_interval,
        'log_every_n_steps': args.log_every_n_steps,
        'callbacks': callbacks,
        'logger': csv_logger,  # æ·»åŠ CSVæ—¥å¿—è®°å½•å™¨
        'precision': args.precision,
        'deterministic': args.deterministic,
        'enable_checkpointing': True,
        'enable_progress_bar': True,
        'enable_model_summary': True,
    }
    
    # è®¾ç½®GPU/è®¾å¤‡
    if args.gpus == -1:
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
        trainer_kwargs['accelerator'] = 'gpu'
        trainer_kwargs['devices'] = 'auto'
    elif args.gpus > 0:
        trainer_kwargs['accelerator'] = 'gpu'
        trainer_kwargs['devices'] = args.gpus
    else:
        trainer_kwargs['accelerator'] = 'cpu'
        trainer_kwargs['devices'] = 1
    
    # è®¾ç½®åˆ†å¸ƒå¼ç­–ç•¥ï¼ˆåŒå¡DDPï¼‰
    if args.gpus > 1 or args.gpus == -1:
        # æ£€æŸ¥å¯ç”¨GPUæ•°é‡
        if torch.cuda.is_available():
            num_gpus = torch.cuda.device_count()
            if num_gpus > 1:
                print(f"ğŸš€ Using DDP strategy with {num_gpus} GPUs")
                trainer_kwargs['strategy'] = DDPStrategy(
                    find_unused_parameters=True,  # å…è®¸æœªä½¿ç”¨çš„å‚æ•°ï¼ˆrealtime encoderå¯èƒ½ä¸æ€»æ˜¯ä½¿ç”¨ï¼‰
                    gradient_as_bucket_view=True,  # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
                )
            else:
                print(f"ğŸ’» Using single GPU")
                trainer_kwargs['strategy'] = 'auto'
        else:
            print("âš ï¸ No GPU available, using CPU")
            trainer_kwargs['accelerator'] = 'cpu'
            trainer_kwargs['devices'] = 1
    
    # è°ƒè¯•é€‰é¡¹
    if args.fast_dev_run:
        trainer_kwargs['fast_dev_run'] = True
        print("âš¡ Running fast dev run (1 batch)")
    
    if args.overfit_batches > 0:
        trainer_kwargs['overfit_batches'] = args.overfit_batches
        print(f"ğŸ”„ Overfitting on {args.overfit_batches} batches")
    
    if args.profile:
        from pytorch_lightning.profilers import PyTorchProfiler
        # ä½¿ç”¨PyTorch Profileræ¥åˆ†ææ€§èƒ½ç“¶é¢ˆ
        trainer_kwargs['profiler'] = PyTorchProfiler(
            dirpath=logs_dir,
            filename="profiler_report",
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
            schedule=torch.profiler.schedule(
                wait=1,
                warmup=1, 
                active=10,
                repeat=2
            ),
            on_trace_ready=torch.profiler.tensorboard_trace_handler(str(logs_dir / "profiler")),
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        )
        print("ğŸ“Š Using PyTorch profiler with CPU/CUDA tracing")
    
    # åˆ›å»ºTrainer
    print("\nğŸ‹ï¸ Creating trainer...")
    trainer = pl.Trainer(**trainer_kwargs)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"\nğŸ“Š Model Information:")
    print(f"  Network: {args.network}")
    print(f"  Flow: {args.flow}")
    print(f"  Hidden dim: {args.hidden_dim}")
    print(f"  Num layers: {args.num_layers}")
    print(f"  Num heads: {args.num_heads}")
    print(f"  Batch size: {args.batch_size}")
    print(f"  Learning rate: {args.lr}")
    print(f"  Scheduler: {args.scheduler}")
    print(f"  æ•°æ®åŠ è½½çº¿ç¨‹: {args.num_workers}")
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ Starting training...")
    if args.resume_from:
        print(f"ğŸ“‚ Resuming from checkpoint: {args.resume_from}")
        trainer.fit(model, data_module, ckpt_path=args.resume_from)
    else:
        trainer.fit(model, data_module)
    
    # æµ‹è¯•ï¼ˆå¦‚æœæœ‰æµ‹è¯•é›†ï¼‰
    if args.test_path:
        print("\nğŸ§ª Running test...")
        trainer.test(model, data_module)
    
    print("\nâœ… Training completed!")
    print(f"ğŸ“ Experiment outputs: {exp_dir}")
    print(f"  â”œâ”€â”€ ğŸ“¦ Checkpoints: {checkpoint_dir}")
    print(f"  â”œâ”€â”€ ğŸ“Š Logs: {logs_dir}")
    print(f"  â””â”€â”€ âš™ï¸  Config: {config_dir}")
    print(f"ğŸ† Best model: {checkpoint_callback.best_model_path}")
    print(f"ğŸ“ˆ Metrics CSV: {logs_dir}/metrics/metrics.csv")


if __name__ == '__main__':
    main()