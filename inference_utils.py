"""
æ¨ç†å·¥å…·å‡½æ•°æ¨¡å—

åŒ…å«æ™¶ä½“ç»“æ„ç”Ÿæˆæ¨ç†æ‰€éœ€çš„æ‰€æœ‰å·¥å…·å‡½æ•°ï¼š
- æ•°æ®åŠ è½½å’Œå¤„ç†
- æ¨¡å‹åŠ è½½å’Œæ¨ç†ï¼ˆæ”¯æŒCFGï¼‰
- PXRDè®¡ç®—å’Œè´¨é‡è¯„ä¼°
- åå¤„ç†å’Œä¼˜åŒ–
- æ–‡ä»¶I/Oæ“ä½œ
"""
import time

import json
import os
import time
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
import torch
from tqdm.auto import tqdm
from pymatgen.core import Structure, Lattice, Composition, Element
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp
import warnings
warnings.filterwarnings('ignore')


# ============== æ•°æ®åŠ è½½å‡½æ•° ==============

def read_xy_file(file_path):
    """
    è¯»å–.xyæ ¼å¼çš„PXRDæ•°æ®
    
    Args:
        file_path: .xyæ–‡ä»¶è·¯å¾„
        
    Returns:
        np.array: PXRDå¼ºåº¦æ•°ç»„
    """
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            if line.startswith('#'):
                continue
            parts = line.strip().split()
            if len(parts) >= 2:
                intensity = float(parts[1])
                data.append(intensity)
    return np.array(data, dtype=np.float32)


def parse_composition(comp_str):
    """
    è§£æç»„æˆå­—ç¬¦ä¸²ä¸ºåŸå­ç±»å‹å’Œæ•°é‡
    
    Args:
        comp_str: ç»„æˆå­—ç¬¦ä¸²ï¼Œå¦‚"Li2 Mn1 O4"
        
    Returns:
        tuple: (åŸå­æ•°é‡, 60ç»´åŸå­ç±»å‹æ•°ç»„)
    """
    comp = Composition(comp_str)
    atom_list = []
    
    for element, count in comp.items():
        atomic_num = Element(element).Z
        atom_list.extend([atomic_num] * int(count))
    
    # å¡«å……åˆ°60ç»´
    atom_types = np.zeros(60, dtype=np.int32)
    atom_types[:len(atom_list)] = atom_list[:60]
    
    return len(atom_list), atom_types


def load_competition_data(data_dir):
    """
    åŠ è½½æ¯”èµ›æ ¼å¼æ•°æ®
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼ŒåŒ…å«composition.jsonå’Œpattern/ç›®å½•
        
    Returns:
        pd.DataFrame: åŒ…å«æ‰€æœ‰æ ·æœ¬ä¿¡æ¯çš„æ•°æ®æ¡†
    """
    data_dir = Path(data_dir)
    
    # è¯»å–composition
    with open(data_dir / "composition.json", 'r') as f:
        compositions = json.load(f)
    
    # å‡†å¤‡æ•°æ®åˆ—è¡¨
    data_list = []
    
    for sample_id, comp_info in tqdm(compositions.items(), desc="åŠ è½½æ•°æ®"):
        # è·å–ç»„æˆä¿¡æ¯
        comp_list = comp_info["composition"]
        niggli_comp = comp_list[0]
        primitive_comp = comp_list[1] if len(comp_list) > 1 else comp_list[0]
        
        # è§£æåŸå­ä¿¡æ¯
        num_atoms, atom_types = parse_composition(niggli_comp)
        
        # è¯»å–PXRDæ•°æ®
        pattern_file = data_dir / "pattern" / f"{sample_id}.xy"
        if pattern_file.exists():
            pxrd = read_xy_file(pattern_file)
            # ç¡®ä¿é•¿åº¦ä¸º11501
            if len(pxrd) < 11501:
                pxrd_full = np.zeros(11501, dtype=np.float32)
                pxrd_full[:len(pxrd)] = pxrd
                pxrd = pxrd_full
            elif len(pxrd) > 11501:
                pxrd = pxrd[:11501]
        else:
            pxrd = np.zeros(11501, dtype=np.float32)
        
        data_list.append({
            'id': sample_id,
            'niggli_comp': niggli_comp,
            'primitive_comp': primitive_comp,
            'atom_types': atom_types,
            'num_atoms': num_atoms,
            'pxrd': pxrd  # è§‚æµ‹çš„PXRDè°±
        })
    
    return pd.DataFrame(data_list)


def load_lattice_stats(stats_path="data/lattice_stats.json"):
    """
    åŠ è½½æ™¶æ ¼å‚æ•°å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        stats_path: ç»Ÿè®¡ä¿¡æ¯JSONæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºdata/lattice_stats.json
        
    Returns:
        dict: åŒ…å«meanå’Œstdçš„ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    stats_path = Path(stats_path)
    
    if not stats_path.exists():
        print(f"âš ï¸ å½’ä¸€åŒ–ç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨: {stats_path}")
        print("  ä½¿ç”¨é»˜è®¤ç»Ÿè®¡å€¼ï¼ˆå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ï¼‰")
        # è¿”å›é»˜è®¤ç»Ÿè®¡å€¼
        return {
            'lattice_mean': np.array([[5.0, 0.0, 0.0],
                                     [0.0, 5.0, 0.0],
                                     [0.0, 0.0, 5.0]], dtype=np.float32),
            'lattice_std': np.array([[2.0, 2.0, 2.0],
                                    [2.0, 2.0, 2.0],
                                    [2.0, 2.0, 2.0]], dtype=np.float32)
        }
    
    with open(stats_path, 'r') as f:
        stats = json.load(f)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    stats_dict = {
        'lattice_mean': np.array(stats.get('lattice_mean', stats.get('mean', [[5,0,0],[0,5,0],[0,0,5]])), dtype=np.float32),
        'lattice_std': np.array(stats.get('lattice_std', stats.get('std', [[2,2,2],[2,2,2],[2,2,2]])), dtype=np.float32)
    }
    
    print(f"âœ… åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
    print(f"   æ™¶æ ¼å‡å€¼èŒƒå›´: [{stats_dict['lattice_mean'].min():.2f}, {stats_dict['lattice_mean'].max():.2f}]")
    print(f"   æ™¶æ ¼æ ‡å‡†å·®èŒƒå›´: [{stats_dict['lattice_std'].min():.2f}, {stats_dict['lattice_std'].max():.2f}]")
    
    return stats_dict


# ============== æ¨¡å‹åŠ è½½å’Œæ¨ç†å‡½æ•° ==============

def load_model(model_path, device='auto'):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæ”¯æŒcfm_cfgæµï¼‰
    
    Args:
        model_path: checkpointæ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡é€‰æ‹© - 'auto'(è‡ªåŠ¨é€‰æ‹©), 'cuda', 'cpu', æˆ–torch.deviceå¯¹è±¡
        
    Returns:
        åŠ è½½å¥½çš„Lightningæ¨¡å—
    """
    from src.trainer import CrystalGenerationModule
    
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    
    # å¤„ç†è®¾å¤‡é€‰æ‹©
    if device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    elif isinstance(device, str):
        device = torch.device(device)
    # å¦‚æœå·²ç»æ˜¯torch.deviceå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
    
    # ä»checkpointåŠ è½½æ¨¡å‹
    model = CrystalGenerationModule.load_from_checkpoint(
        model_path,
        map_location=device
    )
    
    # éªŒè¯æµæ¨¡å‹ç±»å‹
    flow_name = model.hparams.get('flow_name', 'cfm')
    print(f"  æ£€æµ‹åˆ°æµæ¨¡å‹: {flow_name}")
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    model = model.to(device)
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    return model


def initialize_normalizer(stats_path="data/lattice_stats.json", lattice_stats=None):
    """
    åˆå§‹åŒ–æ•°æ®å½’ä¸€åŒ–å™¨
    
    Args:
        stats_path: ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶è·¯å¾„
        lattice_stats: å·²åŠ è½½çš„ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        DataNormalizerå®ä¾‹
    """
    from src.normalizer import DataNormalizer
    
    try:
        # å¦‚æœæä¾›äº†stats_pathï¼Œä½¿ç”¨å®ƒåˆå§‹åŒ–
        if stats_path and Path(stats_path).exists():
            data_normalizer = DataNormalizer(stats_file=stats_path)
            print(f"âœ… ä½¿ç”¨æ–‡ä»¶åˆå§‹åŒ–å½’ä¸€åŒ–å™¨: {stats_path}")
        else:
            # å¦åˆ™åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¹¶åˆå§‹åŒ–
            import tempfile
            import json
            
            # ä½¿ç”¨æä¾›çš„ç»Ÿè®¡ä¿¡æ¯æˆ–é»˜è®¤å€¼
            if lattice_stats is None:
                lattice_stats = load_lattice_stats(stats_path)
            
            # å‡†å¤‡ç¬¦åˆDataNormalizeræœŸæœ›æ ¼å¼çš„ç»Ÿè®¡ä¿¡æ¯
            normalizer_stats = {
                'lattice_global_mean': float(lattice_stats['lattice_mean'].mean()),
                'lattice_global_std': float(lattice_stats['lattice_std'].mean()),
                'lattice_mean': lattice_stats['lattice_mean'].flatten().tolist(),
                'lattice_std': lattice_stats['lattice_std'].flatten().tolist(),
                'frac_coords_mean': [0.5, 0.5, 0.5],  # åˆ†æ•°åæ ‡é»˜è®¤å€¼
                'frac_coords_std': [0.3, 0.3, 0.3]
            }
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(normalizer_stats, f)
                temp_stats_file = f.name
            
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶åˆå§‹åŒ–
            data_normalizer = DataNormalizer(stats_file=temp_stats_file)
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            Path(temp_stats_file).unlink()
            print("âœ… ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯åˆå§‹åŒ–å½’ä¸€åŒ–å™¨")
            
        return data_normalizer
        
    except Exception as e:
        print(f"âš ï¸ å½’ä¸€åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("  å°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼ˆå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ï¼‰")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å½’ä¸€åŒ–å™¨ä½œä¸ºå¤‡ç”¨
        class SimpleNormalizer:
            def __init__(self):
                self.lattice_mean = torch.zeros(3, 3)
                self.lattice_std = torch.ones(3, 3)
            
            def denormalize_z(self, z):
                # ç®€å•çš„åå½’ä¸€åŒ–ï¼šç›´æ¥è¿”å›
                return z * 5.0  # å‡è®¾å…¸å‹æ™¶æ ¼å‚æ•°åœ¨5åŸƒå·¦å³
        
        return SimpleNormalizer()


def generate_crystal_structures_batch_cfg(samples_df, model, data_normalizer, 
                                         batch_size=32, guidance_scale=1.5,
                                         adaptive_mode=False, 
                                         min_scale=0.8, max_scale=2.5):
    """
    æ‰¹é‡ç”Ÿæˆæ™¶ä½“ç»“æ„ï¼ˆä½¿ç”¨CFGå¼•å¯¼ï¼‰
    
    Args:
        samples_df: åŒ…å«å¤šä¸ªæ ·æœ¬çš„DataFrame
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_normalizer: æ•°æ®å½’ä¸€åŒ–å™¨
        batch_size: æ‰¹å¤„ç†å¤§å°
        guidance_scale: CFGå¼•å¯¼å¼ºåº¦ï¼ˆNoneä½¿ç”¨è‡ªé€‚åº”ï¼‰
        adaptive_mode: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”å¼•å¯¼å¼ºåº¦
        min_scale: è‡ªé€‚åº”æ¨¡å¼ä¸‹çš„æœ€å°å¼•å¯¼å¼ºåº¦
        max_scale: è‡ªé€‚åº”æ¨¡å¼ä¸‹çš„æœ€å¤§å¼•å¯¼å¼ºåº¦
    
    Returns:
        tuple: (Structureå¯¹è±¡åˆ—è¡¨, ä½¿ç”¨çš„guidance_scaleåˆ—è¡¨)
    """
    device = next(model.parameters()).device
    structures = []
    scales_used = []
    
    # æŒ‰æ‰¹æ¬¡å¤„ç†
    num_samples = len(samples_df)
    for batch_start in range(0, num_samples, batch_size):
        batch_end = min(batch_start + batch_size, num_samples)
        batch_df = samples_df.iloc[batch_start:batch_end]
        
        # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        batch = {
            'comp': torch.tensor(
                np.stack(batch_df['atom_types'].values), 
                dtype=torch.float32
            ).to(device),
            'pxrd': torch.tensor(
                np.stack(batch_df['pxrd'].values), 
                dtype=torch.float32
            ).to(device),
            'num_atoms': torch.tensor(
                batch_df['num_atoms'].values, 
                dtype=torch.long
            ).to(device),
        }
        
        # è‡ªé€‚åº”é€‰æ‹©å¼•å¯¼å¼ºåº¦
        if adaptive_mode:
            # æ ¹æ®æ ·æœ¬å¤æ‚åº¦ï¼ˆåŸå­æ•°é‡ï¼‰åŠ¨æ€è°ƒæ•´å¼•å¯¼å¼ºåº¦
            complexities = batch_df['num_atoms'].values / 60.0  # å½’ä¸€åŒ–åˆ°[0,1]
            batch_scales = min_scale + (max_scale - min_scale) * complexities
        else:
            batch_scales = [guidance_scale] * len(batch_df)
        
        # å¯¹æ¯ä¸ªä¸åŒçš„scaleå€¼åˆ†ç»„å¤„ç†
        unique_scales = np.unique(batch_scales)
        
        for scale in unique_scales:
            scale_mask = (batch_scales == scale)
            scale_indices = np.where(scale_mask)[0]
            
            if len(scale_indices) == 0:
                continue
            
            # å‡†å¤‡å­æ‰¹æ¬¡
            sub_batch = {
                'comp': batch['comp'][scale_indices],
                'pxrd': batch['pxrd'][scale_indices],
                'num_atoms': batch['num_atoms'][scale_indices],
            }
            
            # ä½¿ç”¨CFGé‡‡æ ·
            print(f"ä½¿ç”¨CFGé‡‡æ ·: {scale}")
            with torch.no_grad():
                generated = model.flow.sample(
                    sub_batch, 
                    guidance_scale=float(scale),
                    temperature=1.0,
                    num_steps=50
                )  # [sub_batch_size, 63, 3]
            
            # åå½’ä¸€åŒ–
            generated_denorm = data_normalizer.denormalize_z(generated)
            generated_denorm = generated_denorm.cpu().numpy()
            
            # å¤„ç†æ¯ä¸ªæ ·æœ¬
            for i, local_idx in enumerate(scale_indices):
                row = batch_df.iloc[local_idx]
                num_atoms = row.num_atoms
                
                # æå–æ™¶æ ¼å’Œåˆ†æ•°åæ ‡
                single_output = generated_denorm[i]  # [63, 3]
                lattice_matrix = single_output[:3, :]  # [3, 3]
                frac_coords = single_output[3:3+num_atoms, :]  # [num_atoms, 3]
                frac_coords = np.mod(frac_coords, 1.0)
                
                # è·å–å…ƒç´ åˆ—è¡¨
                species = []
                for j in range(num_atoms):
                    atomic_num = int(row.atom_types[j])
                    if atomic_num > 0:
                        species.append(Element.from_Z(atomic_num))
                
                # åˆ›å»ºStructureå¯¹è±¡
                try:
                    lattice = Lattice(lattice_matrix)
                    structure = Structure(
                        lattice=lattice,
                        species=species,
                        coords=frac_coords,
                        coords_are_cartesian=False
                    )
                    structures.append(structure)
                    scales_used.append(scale)
                except Exception as e:
                    # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨éšæœºç»“æ„
                    structures.append(generate_random_structure(row.to_dict()))
                    scales_used.append(scale)
    
    return structures, scales_used


def generate_crystal_structures_batch(samples_df, model, data_normalizer, batch_size=32):
    """
    æ‰¹é‡ç”Ÿæˆæ™¶ä½“ç»“æ„ï¼ˆå…¼å®¹æ¥å£ï¼Œä½¿ç”¨é»˜è®¤CFGè®¾ç½®ï¼‰
    
    Args:
        samples_df: åŒ…å«å¤šä¸ªæ ·æœ¬çš„DataFrame
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_normalizer: æ•°æ®å½’ä¸€åŒ–å™¨
        batch_size: æ‰¹å¤„ç†å¤§å°
        
    Returns:
        list: Structureå¯¹è±¡åˆ—è¡¨
    """
    structures, _ = generate_crystal_structures_batch_cfg(
        samples_df, model, data_normalizer, 
        batch_size=batch_size,
        guidance_scale=1.5,
        adaptive_mode=False
    )
    return structures


def generate_random_structure(sample):
    """
    ç”Ÿæˆéšæœºæ™¶ä½“ç»“æ„ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    
    Args:
        sample: æ ·æœ¬æ•°æ®å­—å…¸ï¼ŒåŒ…å«num_atomså’Œatom_types
        
    Returns:
        Structureå¯¹è±¡
    """
    num_atoms = sample['num_atoms']
    
    # éšæœºæ™¶æ ¼å‚æ•°
    a = np.random.uniform(3, 10)
    b = np.random.uniform(3, 10)
    c = np.random.uniform(3, 10)
    alpha = np.random.uniform(60, 120)
    beta = np.random.uniform(60, 120)
    gamma = np.random.uniform(60, 120)
    
    lattice = Lattice.from_parameters(a, b, c, alpha, beta, gamma)
    
    # è·å–å®é™…çš„åŸå­ï¼ˆæ’é™¤paddingçš„0ï¼‰
    atom_types = sample['atom_types']
    species = []
    actual_atom_count = 0
    for i in range(num_atoms):
        atomic_num = int(atom_types[i])
        if atomic_num > 0:
            species.append(Element.from_Z(atomic_num))
            actual_atom_count += 1
    
    # ç”Ÿæˆä¸å®é™…åŸå­æ•°é‡åŒ¹é…çš„åˆ†æ•°åæ ‡
    frac_coords = np.random.rand(actual_atom_count, 3)
    
    return Structure(
        lattice=lattice,
        species=species,
        coords=frac_coords,
        coords_are_cartesian=False
    )


# ============== PXRDè®¡ç®—å’Œè´¨é‡è¯„ä¼° ==============

def calculate_pxrd_worker(structure):
    """
    ç”¨äºå¤šè¿›ç¨‹çš„PXRDè®¡ç®—workerå‡½æ•°ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
    
    Args:
        structure: pymatgen Structureå¯¹è±¡
        
    Returns:
        np.array: 11501ç»´PXRDå¼ºåº¦æ•°ç»„
    """
    import signal
    from contextlib import contextmanager
    
    @contextmanager
    def timeout(seconds):
        """è¶…æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        def timeout_handler(signum, frame):
            raise TimeoutError(f"PXRDè®¡ç®—è¶…æ—¶ ({seconds}ç§’)")
        
        # è®¾ç½®è¶…æ—¶ä¿¡å·
        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(seconds)
        try:
            yield
        finally:
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old_handler)
    
    from src.pxrd_simulator import PXRDSimulator
    simulator = PXRDSimulator()
    start_time = time.time()
    
    try:
        # ä½¿ç”¨2åˆ†é’Ÿè¶…æ—¶
        with timeout(120):
            x_angles, pxrd_intensities = simulator.simulate(structure)
            elapsed = time.time() - start_time
            if elapsed > 10:  # åªæœ‰è¶…è¿‡10ç§’æ‰æ‰“å°è­¦å‘Š
                print(f"âš ï¸ PXRDè®¡ç®—è€—æ—¶è¾ƒé•¿: {elapsed:.2f}ç§’")
            return pxrd_intensities
    except TimeoutError:
        print(f"âŒ PXRDè®¡ç®—è¶…æ—¶ï¼ˆ>120ç§’ï¼‰ï¼Œä½¿ç”¨éšæœºå€¼")
        # è¿”å›éšæœºPXRDä½œä¸ºå¤‡ç”¨
        pxrd_calc = np.random.rand(11501) * 100
        pxrd_calc[pxrd_calc < 10] = 0
        return pxrd_calc
    except Exception as e:
        print(f"âš ï¸ PXRDè®¡ç®—å¤±è´¥: {str(e)[:100]}")
        # è¿”å›éšæœºPXRDä½œä¸ºå¤‡ç”¨
        pxrd_calc = np.random.rand(11501) * 100
        pxrd_calc[pxrd_calc < 10] = 0
        return pxrd_calc


def calculate_pxrd_batch(structures, n_workers=4, timeout_seconds=120):
    """
    æ‰¹é‡è®¡ç®—PXRDè°±ï¼ˆä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œï¼Œå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
    
    Args:
        structures: Structureå¯¹è±¡åˆ—è¡¨
        n_workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
        timeout_seconds: æ¯ä¸ªPXRDè®¡ç®—çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        list: PXRDæ•°ç»„åˆ—è¡¨
    """
    from concurrent.futures import as_completed, TimeoutError as FutureTimeoutError
    
    pxrd_results = [None] * len(structures)
    
    print(f"æ‰¹é‡è®¡ç®— {len(structures)} ä¸ªPXRDè°± (å¹¶è¡Œæ•°={n_workers}, è¶…æ—¶={timeout_seconds}ç§’)")
    
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_index = {}
        for i, structure in enumerate(structures):
            future = executor.submit(calculate_pxrd_worker, structure)
            future_to_index[future] = i
        
        # æ”¶é›†ç»“æœï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰
        completed = 0
        failed = 0
        
        for future in as_completed(future_to_index):
            index = future_to_index[future]
            try:
                # ç­‰å¾…å•ä¸ªä»»åŠ¡å®Œæˆï¼Œè®¾ç½®è¶…æ—¶
                result = future.result(timeout=timeout_seconds)
                pxrd_results[index] = result
                completed += 1
            except FutureTimeoutError:
                print(f"  âŒ æ ·æœ¬ {index} PXRDè®¡ç®—è¶…æ—¶")
                # ä½¿ç”¨éšæœºPXRDä½œä¸ºå¤‡ç”¨
                pxrd_calc = np.random.rand(11501) * 100
                pxrd_calc[pxrd_calc < 10] = 0
                pxrd_results[index] = pxrd_calc
                failed += 1
            except Exception as e:
                print(f"  âš ï¸ æ ·æœ¬ {index} PXRDè®¡ç®—å¤±è´¥: {str(e)[:50]}")
                # ä½¿ç”¨éšæœºPXRDä½œä¸ºå¤‡ç”¨
                pxrd_calc = np.random.rand(11501) * 100
                pxrd_calc[pxrd_calc < 10] = 0
                pxrd_results[index] = pxrd_calc
                failed += 1
            
            # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (completed + failed) % 10 == 0:
                print(f"  è¿›åº¦: {completed + failed}/{len(structures)} (æˆåŠŸ={completed}, å¤±è´¥={failed})")
    
    if failed > 0:
        print(f"âš ï¸ PXRDè®¡ç®—å®Œæˆ: {completed}æˆåŠŸ, {failed}å¤±è´¥ï¼ˆä½¿ç”¨éšæœºå€¼æ›¿ä»£ï¼‰")
    else:
        print(f"âœ… PXRDè®¡ç®—å®Œæˆ: å…¨éƒ¨{completed}ä¸ªæˆåŠŸ")
    
    return pxrd_results


def evaluate_structure_quality(structure, observed_pxrd, pxrd_simulator=None):
    """
    è¯„ä¼°ç”Ÿæˆç»“æ„çš„è´¨é‡
    
    Args:
        structure: ç”Ÿæˆçš„Structureå¯¹è±¡
        observed_pxrd: è§‚æµ‹çš„PXRDè°±
        pxrd_simulator: PXRDä»¿çœŸå™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        float: RWPå€¼ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    """
    if pxrd_simulator is None:
        from src.pxrd_simulator import PXRDSimulator
        pxrd_simulator = PXRDSimulator()
    
    # è®¡ç®—ç”Ÿæˆç»“æ„çš„PXRD
    try:
        x_angles, calculated_pxrd = pxrd_simulator.simulate(structure)
    except:
        # å¤‡ç”¨ï¼šéšæœºPXRD
        calculated_pxrd = np.random.rand(11501) * 100
    
    # è®¡ç®—RWP
    try:
        from src.metrics import rwp
        rwp_value = rwp(calculated_pxrd, observed_pxrd)
    except ImportError:
        # å¤‡ç”¨RWPè®¡ç®—
        diff = calculated_pxrd - observed_pxrd
        weighted_diff = diff * np.sqrt(np.maximum(observed_pxrd, 1e-10))
        rwp_value = np.sqrt(np.sum(weighted_diff**2) / np.sum(observed_pxrd**2 + 1e-10))
    
    return rwp_value


def evaluate_structures_batch(structures, observed_pxrds, n_workers=4, timeout_seconds=120):
    """
    æ‰¹é‡è¯„ä¼°ç»“æ„è´¨é‡ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
    
    Args:
        structures: Structureå¯¹è±¡åˆ—è¡¨
        observed_pxrds: è§‚æµ‹PXRDåˆ—è¡¨
        n_workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
        timeout_seconds: æ¯ä¸ªPXRDè®¡ç®—çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        list: RWPå€¼åˆ—è¡¨
    """
    print(f"å¼€å§‹æ‰¹é‡è¯„ä¼° {len(structures)} ä¸ªç»“æ„...")
    start_time = time.time()
    
    # æ‰¹é‡è®¡ç®—PXRDï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
    calculated_pxrds = calculate_pxrd_batch(
        structures, 
        n_workers=n_workers,
        timeout_seconds=timeout_seconds
    )
    
    # è®¡ç®—RWPå€¼
    rwp_values = []
    failed_count = 0
    
    for i, (calc_pxrd, obs_pxrd) in enumerate(zip(calculated_pxrds, observed_pxrds)):
        try:
            # å°è¯•å¯¼å…¥metricsæ¨¡å—çš„rwpå‡½æ•°
            try:
                from src.metrics import rwp
                rwp_value = rwp(calc_pxrd, obs_pxrd)
            except ImportError:
                # å¤‡ç”¨RWPè®¡ç®—
                diff = calc_pxrd - obs_pxrd
                weighted_diff = diff * np.sqrt(np.maximum(obs_pxrd, 1e-10))
                rwp_value = np.sqrt(np.sum(weighted_diff**2) / np.sum(obs_pxrd**2 + 1e-10))
            
            rwp_values.append(rwp_value)
            
        except Exception as e:
            # å¦‚æœRWPè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ä¸€ä¸ªå¤§å€¼è¡¨ç¤ºè´¨é‡å·®
            print(f"  âš ï¸ æ ·æœ¬ {i} RWPè®¡ç®—å¤±è´¥: {str(e)[:50]}")
            rwp_values.append(999.0)  # ä½¿ç”¨å¤§å€¼è¡¨ç¤ºå¤±è´¥
            failed_count += 1
    
    elapsed = time.time() - start_time
    print(f"è¯„ä¼°å®Œæˆ: è€—æ—¶ {elapsed:.2f}ç§’")
    if failed_count > 0:
        print(f"  âš ï¸ {failed_count} ä¸ªæ ·æœ¬è¯„ä¼°å¤±è´¥ï¼ˆRWP=999ï¼‰")
    
    return rwp_values


# ============== æµæ°´çº¿å¤„ç†å‡½æ•° ==============

def generate_and_evaluate_pipeline(samples_df, model, data_normalizer, observed_pxrds_dict,
                                  batch_size=32, n_workers=4, timeout_seconds=120,
                                  guidance_scale=1.5, adaptive_mode=False):
    """
    æµæ°´çº¿å¤„ç†ï¼šç”Ÿæˆä¸€æ‰¹ç«‹å³è¯„æµ‹ï¼Œé¿å…ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
    
    Args:
        samples_df: åŒ…å«æ ·æœ¬ä¿¡æ¯çš„DataFrame
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_normalizer: æ•°æ®å½’ä¸€åŒ–å™¨
        observed_pxrds_dict: æ ·æœ¬IDåˆ°è§‚æµ‹PXRDçš„å­—å…¸
        batch_size: æ¯æ‰¹ç”Ÿæˆçš„å¤§å°
        n_workers: PXRDè®¡ç®—å¹¶è¡Œæ•°
        timeout_seconds: PXRDè®¡ç®—è¶…æ—¶æ—¶é—´
        guidance_scale: CFGå¼•å¯¼å¼ºåº¦
        adaptive_mode: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”å¼•å¯¼
        
    Returns:
        dict: {sample_id: (structure, rwp_value)}
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import queue
    import threading
    
    results = {}
    num_samples = len(samples_df)
    
    print(f"æµæ°´çº¿å¤„ç† {num_samples} ä¸ªæ ·æœ¬")
    print(f"  æ‰¹å¤§å°: {batch_size}")
    print(f"  PXRDå¹¶è¡Œ: {n_workers} è¿›ç¨‹")
    print(f"  è¶…æ—¶è®¾ç½®: {timeout_seconds}ç§’")
    
    # åˆ›å»ºé˜Ÿåˆ—ç”¨äºæ‰¹æ¬¡é—´é€šä¿¡
    generation_queue = queue.Queue(maxsize=2)  # æœ€å¤šç¼“å­˜2æ‰¹
    
    def generation_worker():
        """ç”Ÿæˆçº¿ç¨‹ï¼šè´Ÿè´£æ‰¹é‡ç”Ÿæˆç»“æ„"""
        for batch_start in range(0, num_samples, batch_size):
            batch_end = min(batch_start + batch_size, num_samples)
            batch_df = samples_df.iloc[batch_start:batch_end]
            
            print(f"\nç”Ÿæˆæ‰¹æ¬¡ [{batch_start}:{batch_end}]...")
            
            # ç”Ÿæˆç»“æ„
            if adaptive_mode:
                structures, scales = generate_crystal_structures_batch_cfg(
                    batch_df, model, data_normalizer,
                    batch_size=batch_size,
                    guidance_scale=guidance_scale,
                    adaptive_mode=True
                )
            else:
                structures = generate_crystal_structures_batch(
                    batch_df, model, data_normalizer,
                    batch_size=batch_size
                )
                scales = [guidance_scale] * len(structures)
            
            # å°†ç”Ÿæˆçš„ç»“æ„æ”¾å…¥é˜Ÿåˆ—
            batch_data = {
                'df': batch_df,
                'structures': structures,
                'scales': scales if adaptive_mode else None,
                'batch_idx': (batch_start, batch_end)
            }
            generation_queue.put(batch_data)
            print(f"  âœ“ æ‰¹æ¬¡ [{batch_start}:{batch_end}] ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹è¯„æµ‹...")
        
        # æ ‡è®°ç”Ÿæˆå®Œæˆ
        generation_queue.put(None)
    
    def evaluation_worker():
        """è¯„æµ‹çº¿ç¨‹ï¼šä»é˜Ÿåˆ—è·å–ç»“æ„å¹¶è¯„æµ‹"""
        batch_count = 0
        total_evaluated = 0
        
        while True:
            # ä»é˜Ÿåˆ—è·å–æ‰¹æ¬¡æ•°æ®
            batch_data = generation_queue.get()
            if batch_data is None:  # ç”Ÿæˆå®Œæˆ
                break
            
            batch_df = batch_data['df']
            structures = batch_data['structures']
            batch_start, batch_end = batch_data['batch_idx']
            batch_count += 1
            
            print(f"\nè¯„æµ‹æ‰¹æ¬¡ {batch_count} [{batch_start}:{batch_end}]...")
            
            # è·å–è¯¥æ‰¹æ¬¡çš„è§‚æµ‹PXRD
            observed_pxrds = []
            for sample_id in batch_df['id']:
                if sample_id in observed_pxrds_dict:
                    observed_pxrds.append(observed_pxrds_dict[sample_id])
                else:
                    # å¦‚æœæ²¡æœ‰è§‚æµ‹æ•°æ®ï¼Œä½¿ç”¨DataFrameä¸­çš„
                    idx = batch_df[batch_df['id'] == sample_id].index[0]
                    observed_pxrds.append(batch_df.loc[idx, 'pxrd'])
            
            # æ‰¹é‡è¯„æµ‹ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
            rwp_values = evaluate_structures_batch(
                structures, observed_pxrds,
                n_workers=n_workers,
                timeout_seconds=timeout_seconds
            )
            
            # ä¿å­˜ç»“æœ
            for i, (sample_id, structure, rwp) in enumerate(
                zip(batch_df['id'], structures, rwp_values)
            ):
                results[sample_id] = {
                    'structure': structure,
                    'rwp': rwp,
                    'scale': batch_data['scales'][i] if batch_data['scales'] else guidance_scale
                }
                total_evaluated += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            print(f"  âœ“ æ‰¹æ¬¡ {batch_count} è¯„æµ‹å®Œæˆ")
            print(f"  æ€»è¿›åº¦: {total_evaluated}/{num_samples} ({total_evaluated/num_samples*100:.1f}%)")
            
            # æ˜¾ç¤ºè´¨é‡ç»Ÿè®¡
            batch_rwps = [r for r in rwp_values if r < 999]
            if batch_rwps:
                print(f"  æ‰¹æ¬¡RWP: æœ€å°={min(batch_rwps):.4f}, å¹³å‡={np.mean(batch_rwps):.4f}, æœ€å¤§={max(batch_rwps):.4f}")
    
    # å¯åŠ¨ä¸¤ä¸ªçº¿ç¨‹
    print("\nå¯åŠ¨æµæ°´çº¿...")
    generation_thread = threading.Thread(target=generation_worker, name="Generation")
    evaluation_thread = threading.Thread(target=evaluation_worker, name="Evaluation")
    
    generation_thread.start()
    evaluation_thread.start()
    
    # ç­‰å¾…å®Œæˆ
    generation_thread.join()
    evaluation_thread.join()
    
    print(f"\nâœ… æµæ°´çº¿å®Œæˆï¼å…±å¤„ç† {len(results)} ä¸ªæ ·æœ¬")
    
    # ç»Ÿè®¡ç»“æœ
    all_rwps = [r['rwp'] for r in results.values() if r['rwp'] < 999]
    if all_rwps:
        print(f"æ•´ä½“RWPç»Ÿè®¡:")
        print(f"  æœ€å°: {min(all_rwps):.4f}")
        print(f"  å¹³å‡: {np.mean(all_rwps):.4f}")
        print(f"  ä¸­ä½æ•°: {np.median(all_rwps):.4f}")
        print(f"  æœ€å¤§: {max(all_rwps):.4f}")
    
    return results


def generate_and_evaluate_batch_simple(samples_df, model, data_normalizer,
                                      batch_size=32, n_workers=4, timeout_seconds=120):
    """
    ç®€åŒ–ç‰ˆæ‰¹é‡ç”Ÿæˆå’Œè¯„æµ‹ï¼ˆé¡ºåºå¤„ç†æ¯ä¸ªæ‰¹æ¬¡ï¼‰
    
    Args:
        samples_df: åŒ…å«æ ·æœ¬ä¿¡æ¯çš„DataFrameï¼ˆå¿…é¡»åŒ…å«'pxrd'åˆ—ï¼‰
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_normalizer: æ•°æ®å½’ä¸€åŒ–å™¨
        batch_size: æ‰¹å¤„ç†å¤§å°
        n_workers: PXRDè®¡ç®—å¹¶è¡Œæ•°
        timeout_seconds: PXRDè®¡ç®—è¶…æ—¶æ—¶é—´
        
    Returns:
        tuple: (structuresåˆ—è¡¨, rwp_valuesåˆ—è¡¨)
    """
    all_structures = []
    all_rwp_values = []
    num_samples = len(samples_df)
    
    print(f"æ‰¹é‡å¤„ç† {num_samples} ä¸ªæ ·æœ¬ï¼ˆæ‰¹å¤§å°={batch_size}ï¼‰")
    
    # æŒ‰æ‰¹æ¬¡å¤„ç†
    for batch_idx, batch_start in enumerate(range(0, num_samples, batch_size), 1):
        batch_end = min(batch_start + batch_size, num_samples)
        batch_df = samples_df.iloc[batch_start:batch_end]
        
        print(f"\næ‰¹æ¬¡ {batch_idx}: [{batch_start}:{batch_end}]")
        
        # 1. ç”Ÿæˆç»“æ„
        print(f"  ç”Ÿæˆç»“æ„...")
        batch_structures = generate_crystal_structures_batch(
            batch_df, model, data_normalizer, batch_size=len(batch_df)
        )
        
        # 2. ç«‹å³è¯„æµ‹
        print(f"  è¯„æµ‹è´¨é‡...")
        batch_pxrds = batch_df['pxrd'].tolist()
        batch_rwps = evaluate_structures_batch(
            batch_structures, batch_pxrds,
            n_workers=n_workers,
            timeout_seconds=timeout_seconds
        )
        
        # 3. ä¿å­˜ç»“æœ
        all_structures.extend(batch_structures)
        all_rwp_values.extend(batch_rwps)
        
        # 4. æ˜¾ç¤ºæ‰¹æ¬¡ç»Ÿè®¡
        valid_rwps = [r for r in batch_rwps if r < 999]
        if valid_rwps:
            print(f"  æ‰¹æ¬¡ç»Ÿè®¡: RWPå‡å€¼={np.mean(valid_rwps):.4f}, æœ€å°={min(valid_rwps):.4f}")
        
        print(f"  ç´¯è®¡è¿›åº¦: {len(all_structures)}/{num_samples} ({len(all_structures)/num_samples*100:.1f}%)")
    
    print(f"\nâœ… å®Œæˆï¼å…±ç”Ÿæˆå¹¶è¯„æµ‹ {len(all_structures)} ä¸ªç»“æ„")
    
    return all_structures, all_rwp_values


# ============== åå¤„ç†å‡½æ•° ==============

def energy_optimization(structure):
    """
    èƒ½é‡ä¼˜åŒ–ï¼ˆå ä½å®ç°ï¼‰
    
    Args:
        structure: å¾…ä¼˜åŒ–çš„Structureå¯¹è±¡
    
    Returns:
        Structure: ä¼˜åŒ–åçš„ç»“æ„
    """
    # TODO: å®ç°çœŸå®çš„èƒ½é‡ä¼˜åŒ–ï¼ˆGULPã€VASPç­‰ï¼‰
    
    # å ä½ï¼šç¨å¾®è°ƒæ•´æ™¶æ ¼å‚æ•°æ¨¡æ‹Ÿä¼˜åŒ–
    new_lattice = structure.lattice.matrix * np.random.uniform(0.98, 1.02)
    optimized = Structure(
        lattice=Lattice(new_lattice),
        species=structure.species,
        coords=structure.frac_coords,
        coords_are_cartesian=False
    )
    
    return optimized


def rietveld_refinement(structure, observed_pxrd, rwp_threshold=0.15):
    """
    Rietveldç²¾ä¿®ï¼ˆå ä½å®ç°ï¼‰
    
    Args:
        structure: å¾…ç²¾ä¿®çš„Structureå¯¹è±¡
        observed_pxrd: è§‚æµ‹çš„PXRDè°±
        rwp_threshold: RWPé˜ˆå€¼
    
    Returns:
        tuple: (ç²¾ä¿®åçš„Structure, æ˜¯å¦è¿›è¡Œäº†ç²¾ä¿®)
    """
    # åˆ¤æ–­æ˜¯å¦éœ€è¦ç²¾ä¿®
    current_rwp = evaluate_structure_quality(structure, observed_pxrd)
    needs_refinement = current_rwp > rwp_threshold * 1.5
    
    if not needs_refinement:
        return structure, False
    
    # TODO: å®ç°çœŸå®çš„Rietveldç²¾ä¿®ï¼ˆGSAS-IIã€TOPASç­‰ï¼‰
    
    # å ä½ï¼šç¨å¾®è°ƒæ•´åŸå­ä½ç½®æ¨¡æ‹Ÿç²¾ä¿®
    new_coords = structure.frac_coords + np.random.randn(*structure.frac_coords.shape) * 0.01
    new_coords = np.clip(new_coords, 0, 1)
    
    refined = Structure(
        lattice=structure.lattice,
        species=structure.species,
        coords=new_coords,
        coords_are_cartesian=False
    )
    
    return refined, True


def post_process_structure(structure, observed_pxrd, rwp_threshold=0.15):
    """
    å®Œæ•´çš„åå¤„ç†æµç¨‹
    
    Args:
        structure: å¾…å¤„ç†çš„Structureå¯¹è±¡
        observed_pxrd: è§‚æµ‹çš„PXRDè°±
        rwp_threshold: RWPé˜ˆå€¼
    
    Returns:
        tuple: (å¤„ç†åçš„Structure, æœ€ç»ˆRWPå€¼)
    """
    # 1. èƒ½é‡ä¼˜åŒ–
    optimized = energy_optimization(structure)
    rwp_after_opt = evaluate_structure_quality(optimized, observed_pxrd)
    
    # 2. Rietveldç²¾ä¿®ï¼ˆå¦‚æœéœ€è¦ï¼‰
    refined, was_refined = rietveld_refinement(optimized, observed_pxrd, rwp_threshold)
    
    if was_refined:
        rwp_after_refine = evaluate_structure_quality(refined, observed_pxrd)
        return refined, rwp_after_refine
    else:
        return optimized, rwp_after_opt


# ============== è¿­ä»£ä¼˜åŒ–æ§åˆ¶å‡½æ•° ==============

def check_termination_conditions(sample_status, start_time, max_runtime, max_attempts):
    """
    æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç»ˆæ­¢æ¡ä»¶
    
    Args:
        sample_status: æ ·æœ¬çŠ¶æ€å­—å…¸
        start_time: å¼€å§‹æ—¶é—´æˆ³
        max_runtime: æœ€å¤§è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
        max_attempts: å•æ ·æœ¬æœ€å¤§å°è¯•æ¬¡æ•°
    
    Returns:
        tuple: (æ˜¯å¦ç»ˆæ­¢, ç»ˆæ­¢åŸå› )
    """
    # æ£€æŸ¥è¿è¡Œæ—¶é—´
    elapsed_time = time.time() - start_time
    if elapsed_time > max_runtime:
        return True, f"è¾¾åˆ°æœ€å¤§è¿è¡Œæ—¶é—´ {max_runtime/3600:.1f} å°æ—¶"
    
    # æ£€æŸ¥æ‰€æœ‰æ ·æœ¬çŠ¶æ€
    all_done = all(
        status['satisfied'] or status['attempts'] >= max_attempts
        for status in sample_status.values()
    )
    
    if all_done:
        satisfied_count = sum(1 for s in sample_status.values() if s['satisfied'])
        return True, f"æ‰€æœ‰æ ·æœ¬å¤„ç†å®Œæˆï¼ˆ{satisfied_count}/{len(sample_status)}æ»¡è¶³è¦æ±‚ï¼‰"
    
    return False, None


def get_samples_to_regenerate(sample_status, batch_size=32, max_attempts=10):
    """
    è·å–éœ€è¦é‡æ–°ç”Ÿæˆçš„æ ·æœ¬
    
    Args:
        sample_status: æ ·æœ¬çŠ¶æ€å­—å…¸
        batch_size: æ‰¹æ¬¡å¤§å°
        max_attempts: å•æ ·æœ¬æœ€å¤§å°è¯•æ¬¡æ•°
    
    Returns:
        list: éœ€è¦é‡æ–°ç”Ÿæˆçš„æ ·æœ¬IDåˆ—è¡¨
    """
    # æ‰¾å‡ºæœªæ»¡è¶³è¦æ±‚ä¸”æœªè¶…è¿‡å°è¯•æ¬¡æ•°çš„æ ·æœ¬
    candidates = [
        sample_id for sample_id, status in sample_status.items()
        if not status['satisfied'] and status['attempts'] < max_attempts
    ]
    
    # æŒ‰RWPå€¼æ’åºï¼Œä¼˜å…ˆå¤„ç†è´¨é‡æœ€å·®çš„
    candidates.sort(key=lambda x: sample_status[x]['best_rwp'], reverse=True)
    
    return candidates[:batch_size]


def get_adaptive_cfg_scale(iteration, base_scale=1.5, min_scale=0.8, max_scale=2.5):
    """
    æ ¹æ®è¿­ä»£æ¬¡æ•°è·å–è‡ªé€‚åº”çš„CFGå¼•å¯¼å¼ºåº¦
    
    Args:
        iteration: å½“å‰è¿­ä»£è½®æ¬¡
        base_scale: åŸºç¡€å¼•å¯¼å¼ºåº¦
        min_scale: æœ€å°å¼•å¯¼å¼ºåº¦
        max_scale: æœ€å¤§å¼•å¯¼å¼ºåº¦
    
    Returns:
        float: æ¨èçš„CFGå¼•å¯¼å¼ºåº¦
    """
    if iteration <= 2:
        # æ—©æœŸï¼šæ ‡å‡†å¼•å¯¼
        return base_scale
    elif iteration <= 5:
        # ä¸­æœŸï¼šå¢å¼ºå¼•å¯¼
        return min(base_scale * 1.5, max_scale)
    else:
        # åæœŸï¼šé™ä½å¼•å¯¼å¢åŠ å¤šæ ·æ€§
        return max(base_scale * 0.8, min_scale)


# ============== æ–‡ä»¶I/Oå‡½æ•° ==============

def update_submission_incrementally(sample_status, data_dir, output_file="submission.csv"):
    """
    å¢é‡æ›´æ–°submission.csvæ–‡ä»¶
    
    Args:
        sample_status: æ ·æœ¬çŠ¶æ€å­—å…¸ï¼ŒåŒ…å«æ¯ä¸ªæ ·æœ¬çš„æœ€ä½³ç»“æ„
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶å
    
    Returns:
        pd.DataFrame: submissionæ•°æ®æ¡†
    """
    # å‡†å¤‡submissionæ•°æ®
    rows = []
    
    for sample_id, status in sample_status.items():
        try:
            structure = status['best_structure']
            
            if structure is not None:
                # è½¬æ¢ä¸ºCIFæ ¼å¼
                cif_str = structure.to(fmt="cif")
            else:
                # å¦‚æœè¿˜æ²¡æœ‰ç»“æ„ï¼Œåˆ›å»ºå ä½CIF
                cif_str = f"data_{sample_id}\n_cell_length_a 5.0\n_cell_length_b 5.0\n_cell_length_c 5.0\n_cell_angle_alpha 90\n_cell_angle_beta 90\n_cell_angle_gamma 90\n"
            
            rows.append({
                'ID': sample_id,
                'cif': cif_str
            })
        except Exception as e:
            # å‡ºé”™æ—¶åˆ›å»ºå ä½CIF
            min_cif = f"data_{sample_id}\n_cell_length_a 5.0\n_cell_length_b 5.0\n_cell_length_c 5.0\n_cell_angle_alpha 90\n_cell_angle_beta 90\n_cell_angle_gamma 90\n"
            rows.append({
                'ID': sample_id,
                'cif': min_cif
            })
    
    # åˆ›å»ºDataFrame
    submission_df = pd.DataFrame(rows)
    
    # ä¿å­˜ä¸ºCSVï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰
    submission_df.to_csv(output_file, index=False)
    
    return submission_df


def log_submission_update(iteration, sample_status, submission_file="submission.csv"):
    """
    è®°å½•submissionæ›´æ–°ä¿¡æ¯
    
    Args:
        iteration: å½“å‰è¿­ä»£è½®æ¬¡ï¼ˆ0è¡¨ç¤ºåˆå§‹æ¨ç†ï¼‰
        sample_status: æ ·æœ¬çŠ¶æ€å­—å…¸
        submission_file: submissionæ–‡ä»¶è·¯å¾„
    """
    satisfied_count = sum(1 for s in sample_status.values() if s['satisfied'])
    total_count = len(sample_status)
    
    if iteration == 0:
        print(f"\nğŸ“ åˆå§‹submission.csvå·²ç”Ÿæˆ")
    else:
        print(f"\nğŸ“ submission.csvå·²æ›´æ–° (è¿­ä»£{iteration})")
    
    print(f"   æ»¡è¶³è¦æ±‚: {satisfied_count}/{total_count} ({satisfied_count/total_count*100:.1f}%)")
    
    if os.path.exists(submission_file):
        file_size = os.path.getsize(submission_file) / 1024
        print(f"   æ–‡ä»¶å¤§å°: {file_size:.2f} KB")


def print_final_statistics(sample_status, start_time, rwp_threshold=0.15, max_attempts=10):
    """
    æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    
    Args:
        sample_status: æ ·æœ¬çŠ¶æ€å­—å…¸
        start_time: å¼€å§‹æ—¶é—´æˆ³
        rwp_threshold: RWPé˜ˆå€¼
        max_attempts: å•æ ·æœ¬æœ€å¤§å°è¯•æ¬¡æ•°
    """
    print("\n" + "="*60)
    print("æœ€ç»ˆç»Ÿè®¡")
    print("="*60)
    
    # è®¡ç®—å„é¡¹ç»Ÿè®¡æŒ‡æ ‡
    satisfied_samples = [s for s in sample_status.values() if s['satisfied']]
    unsatisfied_samples = [s for s in sample_status.values() if not s['satisfied']]
    
    print(f"\nè´¨é‡ç»Ÿè®¡:")
    print(f"  æ»¡è¶³RWP<{rwp_threshold}: {len(satisfied_samples)}/{len(sample_status)} ({len(satisfied_samples)/len(sample_status)*100:.1f}%)")
    print(f"  æœªæ»¡è¶³è¦æ±‚: {len(unsatisfied_samples)}")
    
    if satisfied_samples:
        satisfied_rwps = [s['best_rwp'] for s in satisfied_samples]
        print(f"\næ»¡è¶³è¦æ±‚æ ·æœ¬çš„RWP:")
        print(f"  æœ€å°: {np.min(satisfied_rwps):.4f}")
        print(f"  æœ€å¤§: {np.max(satisfied_rwps):.4f}")
        print(f"  å¹³å‡: {np.mean(satisfied_rwps):.4f}")
    
    if unsatisfied_samples:
        unsatisfied_rwps = [s['best_rwp'] for s in unsatisfied_samples]
        print(f"\næœªæ»¡è¶³è¦æ±‚æ ·æœ¬çš„RWP:")
        print(f"  æœ€å°: {np.min(unsatisfied_rwps):.4f}")
        print(f"  æœ€å¤§: {np.max(unsatisfied_rwps):.4f}")
        print(f"  å¹³å‡: {np.mean(unsatisfied_rwps):.4f}")
    
    # å°è¯•æ¬¡æ•°ç»Ÿè®¡
    attempts_list = [s['attempts'] for s in sample_status.values()]
    print(f"\nå°è¯•æ¬¡æ•°ç»Ÿè®¡:")
    print(f"  æœ€å°‘: {np.min(attempts_list)}")
    print(f"  æœ€å¤š: {np.max(attempts_list)}")
    print(f"  å¹³å‡: {np.mean(attempts_list):.1f}")
    print(f"  è¾¾åˆ°ä¸Šé™({max_attempts}æ¬¡): {sum(1 for a in attempts_list if a >= max_attempts)}")
    
    # è¿è¡Œæ—¶é—´
    total_time = time.time() - start_time
    print(f"\næ€»è¿è¡Œæ—¶é—´: {total_time/3600:.2f}å°æ—¶")