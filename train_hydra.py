#!/usr/bin/env python
"""
åŸºäºHydraé…ç½®ç®¡ç†çš„æ™¶ä½“ç»“æ„ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬
æ”¯æŒé…ç½®ç»„åˆã€å‚æ•°è¦†ç›–å’Œå®éªŒç®¡ç†
"""

import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import (
    ModelCheckpoint, EarlyStopping, LearningRateMonitor, RichProgressBar
)
from pytorch_lightning.strategies import DDPStrategy
from pytorch_lightning.loggers import CSVLogger, WandbLogger
from omegaconf import DictConfig, OmegaConf
import hydra
from hydra.core.config_store import ConfigStore
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
import warnings

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.trainer import CrystalGenerationModule, CrystalGenerationDataModule
from src.ema_callback import EMACallback

# å¿½ç•¥ä¸€äº›æ— å®³çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='torch.nn.parallel')

console = Console()


def print_config(cfg: DictConfig) -> None:
    """ç¾è§‚åœ°æ‰“å°é…ç½®ä¿¡æ¯"""
    console.print("\n[bold cyan]â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[/bold cyan]")
    console.print("[bold cyan]                    è®­ç»ƒé…ç½®                                [/bold cyan]")
    console.print("[bold cyan]â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[/bold cyan]\n")
    
    # ç½‘ç»œé…ç½®
    network_table = Table(title="ğŸ§  ç½‘ç»œé…ç½®", show_header=True, header_style="bold magenta")
    network_table.add_column("å‚æ•°", style="cyan", width=20)
    network_table.add_column("å€¼", style="green")
    
    network_table.add_row("ç½‘ç»œç±»å‹", cfg.networks.name)
    for key, value in cfg.networks.config.items():
        network_table.add_row(f"  {key}", str(value))
    
    console.print(network_table)
    console.print()
    
    # æµæ¨¡å‹é…ç½®
    flow_table = Table(title="ğŸŒŠ æµæ¨¡å‹é…ç½®", show_header=True, header_style="bold magenta")
    flow_table.add_column("å‚æ•°", style="cyan", width=20)
    flow_table.add_column("å€¼", style="green")
    
    flow_table.add_row("æµæ¨¡å‹ç±»å‹", cfg.flows.name)
    for key, value in cfg.flows.config.items():
        if isinstance(value, float):
            flow_table.add_row(f"  {key}", f"{value:.6f}")
        else:
            flow_table.add_row(f"  {key}", str(value))
    
    console.print(flow_table)
    console.print()
    
    # æ•°æ®é…ç½®
    data_table = Table(title="ğŸ“Š æ•°æ®é…ç½®", show_header=True, header_style="bold magenta")
    data_table.add_column("å‚æ•°", style="cyan", width=20)
    data_table.add_column("å€¼", style="green")
    
    data_table.add_row("è®­ç»ƒè·¯å¾„", cfg.data.train_path)
    data_table.add_row("éªŒè¯è·¯å¾„", cfg.data.val_path)
    data_table.add_row("æµ‹è¯•è·¯å¾„", str(cfg.data.test_path))
    data_table.add_row("æ‰¹æ¬¡å¤§å°", str(cfg.data.batch_size))
    data_table.add_row("å·¥ä½œçº¿ç¨‹", str(cfg.data.num_workers))
    data_table.add_row("ç½®æ¢å¢å¼º", f"{cfg.data.augmentation.permutation.enabled} (p={cfg.data.augmentation.permutation.prob})")
    data_table.add_row("SO3å¢å¼º", f"{cfg.data.augmentation.so3.enabled} (p={cfg.data.augmentation.so3.prob})")
    
    console.print(data_table)
    console.print()
    
    # è®­ç»ƒé…ç½®
    trainer_table = Table(title="ğŸ¯ è®­ç»ƒé…ç½®", show_header=True, header_style="bold magenta")
    trainer_table.add_column("å‚æ•°", style="cyan", width=20)
    trainer_table.add_column("å€¼", style="green")
    
    trainer_table.add_row("æœ€å¤§è½®æ•°", str(cfg.trainer.max_epochs))
    trainer_table.add_row("å­¦ä¹ ç‡", f"{cfg.optimizer.lr:.2e}")
    trainer_table.add_row("ä¼˜åŒ–å™¨", cfg.optimizer.type)
    trainer_table.add_row("è°ƒåº¦å™¨", cfg.scheduler.type)
    trainer_table.add_row("æ¢¯åº¦è£å‰ª", str(cfg.trainer.gradient_clip_val))
    trainer_table.add_row("æ¢¯åº¦ç´¯ç§¯", str(cfg.trainer.accumulate_grad_batches))
    trainer_table.add_row("ç²¾åº¦", cfg.trainer.precision)
    trainer_table.add_row("è®¾å¤‡", f"{cfg.trainer.accelerator} ({cfg.trainer.devices})")
    
    console.print(trainer_table)
    console.print()


def setup_callbacks(cfg: DictConfig, checkpoint_dir: Path, use_wandb: bool = False) -> list:
    """è®¾ç½®è®­ç»ƒå›è°ƒ"""
    callbacks = []
    
    # Richè¿›åº¦æ¡
    callbacks.append(RichProgressBar(refresh_rate=10))
    
    # EMAå›è°ƒ
    if cfg.ema.use_ema:
        ema_callback = EMACallback(decay=cfg.ema.decay, update_every=cfg.ema.update_every)
        callbacks.append(ema_callback)
        console.print(f"âœ¨ EMAå·²å¯ç”¨ (decay={cfg.ema.decay})")
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename='epoch={epoch:03d}-val_loss={val/loss:.4f}',
        monitor=cfg.checkpoint.monitor,
        mode=cfg.checkpoint.mode,
        save_top_k=cfg.checkpoint.save_top_k,
        save_last=cfg.checkpoint.save_last,
        verbose=True,
    )
    callbacks.append(checkpoint_callback)
    
    # æ—©åœå›è°ƒ
    if cfg.early_stopping.patience > 0:
        early_stopping = EarlyStopping(
            monitor=cfg.early_stopping.monitor,
            patience=cfg.early_stopping.patience,
            mode=cfg.early_stopping.mode,
            verbose=True,
        )
        callbacks.append(early_stopping)
    
    # å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval='step')
    callbacks.append(lr_monitor)
    
    return callbacks


def setup_loggers(cfg: DictConfig, logs_dir: Path) -> list:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    loggers = []
    
    # CSVæ—¥å¿—è®°å½•å™¨
    csv_logger = CSVLogger(
        save_dir=logs_dir,
        name="metrics",
        version=""
    )
    loggers.append(csv_logger)
    
    # WandBæ—¥å¿—è®°å½•å™¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    if cfg.get('wandb', {}).get('enabled', False):
        wandb_logger = WandbLogger(
            project=cfg.wandb.get('project', 'crystal-generation'),
            name=cfg.experiment.name,
            save_dir=logs_dir,
            log_model=cfg.wandb.get('log_model', False),
            config=OmegaConf.to_container(cfg, resolve=True),
        )
        loggers.append(wandb_logger)
        console.print("ğŸ“Š WandBæ—¥å¿—è®°å½•å·²å¯ç”¨")
    
    return loggers


def validate_paths(cfg: DictConfig) -> bool:
    """éªŒè¯å¿…è¦çš„è·¯å¾„æ˜¯å¦å­˜åœ¨"""
    errors = []
    
    # è·å–åŸå§‹å·¥ä½œç›®å½•ï¼ˆHydraæ”¹å˜ç›®å½•å‰çš„è·¯å¾„ï¼‰
    from hydra.core.hydra_config import HydraConfig
    orig_cwd = HydraConfig.get().runtime.cwd
    
    # æ£€æŸ¥è®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨åŸå§‹å·¥ä½œç›®å½•çš„ç›¸å¯¹è·¯å¾„ï¼‰
    train_path = Path(orig_cwd) / cfg.data.train_path
    if not train_path.exists():
        errors.append(f"è®­ç»ƒç¼“å­˜æœªæ‰¾åˆ°: {train_path}")
        errors.append(f"  è¯·è¿è¡Œ: python scripts/warmup_cache.py --csv your_train.csv --output_dir {cfg.data.train_path}")
    
    # æ£€æŸ¥éªŒè¯æ•°æ®
    val_path = Path(orig_cwd) / cfg.data.val_path
    if not val_path.exists():
        errors.append(f"éªŒè¯ç¼“å­˜æœªæ‰¾åˆ°: {val_path}")
        errors.append(f"  è¯·è¿è¡Œ: python scripts/warmup_cache.py --csv your_val.csv --output_dir {cfg.data.val_path}")
    
    # æ£€æŸ¥ç»Ÿè®¡æ–‡ä»¶
    stats_file = Path(orig_cwd) / cfg.flows.config.stats_file
    if not stats_file.exists():
        errors.append(f"ç»Ÿè®¡æ–‡ä»¶æœªæ‰¾åˆ°: {stats_file}")
        errors.append(f"  è¯·è¿è¡Œ: python scripts/calculate_lattice_stats.py --cache_dirs {cfg.data.train_path} {cfg.data.val_path} --output {cfg.flows.config.stats_file}")
    
    # æ£€æŸ¥MatScholaråµŒå…¥æ–‡ä»¶ï¼ˆå¦‚æœä½¿ç”¨V2ç½‘ç»œï¼‰
    if cfg.networks.name == "crystal_transformer_v2":
        matscholar_path = cfg.networks.config.get('matscholar_path', '')
        if matscholar_path:
            matscholar_full_path = Path(orig_cwd) / matscholar_path
            if not matscholar_full_path.exists():
                errors.append(f"MatScholaråµŒå…¥æ–‡ä»¶æœªæ‰¾åˆ°: {matscholar_full_path}")
    
    if errors:
        console.print("\n[bold red]âŒ è·¯å¾„éªŒè¯å¤±è´¥ï¼š[/bold red]")
        for error in errors:
            console.print(f"  {error}")
        return False
    
    return True


@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig) -> None:
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(cfg.experiment.seed, workers=True)
    
    # ç”Ÿæˆå®éªŒåç§°
    if cfg.experiment.name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        cfg.experiment.name = f"{cfg.networks.name}_{cfg.flows.name}_{timestamp}"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    exp_dir = Path.cwd()  # Hydraå·²ç»åˆ›å»ºäº†å·¥ä½œç›®å½•
    checkpoint_dir = exp_dir / "checkpoints"
    logs_dir = exp_dir / "logs"
    config_dir = exp_dir / "config"
    
    # æ‰“å°Hydraç”Ÿæˆçš„è¾“å‡ºç›®å½•
    console.print(f"\n[bold green]å®éªŒç›®å½•: {exp_dir}[/bold green]")
    
    checkpoint_dir.mkdir(exist_ok=True)
    logs_dir.mkdir(exist_ok=True)
    config_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜å®Œæ•´é…ç½®
    config_path = config_dir / "config.yaml"
    OmegaConf.save(cfg, config_path)
    
    # æ‰“å°é…ç½®
    print_config(cfg)
    
    # éªŒè¯è·¯å¾„
    if not validate_paths(cfg):
        sys.exit(1)
    
    console.print(Panel.fit(
        f"[bold cyan]å®éªŒ: {cfg.experiment.name}[/bold cyan]\n"
        f"ğŸ“ æ£€æŸ¥ç‚¹: {checkpoint_dir}\n"
        f"ğŸ“Š æ—¥å¿—: {logs_dir}\n"
        f"âš™ï¸  é…ç½®: {config_dir}",
        title="ğŸš€ å¼€å§‹è®­ç»ƒ",
        border_style="cyan"
    ))
    
    # è·å–åŸå§‹å·¥ä½œç›®å½•
    from hydra.core.hydra_config import HydraConfig
    orig_cwd = HydraConfig.get().runtime.cwd
    
    # å‡†å¤‡é…ç½®å­—å…¸ï¼Œä¿®æ­£è·¯å¾„
    network_config = OmegaConf.to_container(cfg.networks.config, resolve=True)
    flow_config = OmegaConf.to_container(cfg.flows.config, resolve=True)
    
    # ä¿®æ­£æ–‡ä»¶è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
    if 'stats_file' in flow_config:
        flow_config['stats_file'] = str(Path(orig_cwd) / flow_config['stats_file'])
    if 'matscholar_path' in network_config:
        network_config['matscholar_path'] = str(Path(orig_cwd) / network_config['matscholar_path'])
    
    optimizer_config = {
        'type': cfg.optimizer.type,
        'lr': cfg.optimizer.lr,
        'weight_decay': cfg.optimizer.weight_decay,
        'betas': cfg.optimizer.betas,
    }
    
    scheduler_config = None
    if cfg.scheduler.type != 'none':
        scheduler_config = OmegaConf.to_container(cfg.scheduler, resolve=True)
    
    # åˆ›å»ºæ¨¡å‹
    console.print(f"\nğŸ“Š åˆ›å»ºæ¨¡å‹: {cfg.networks.name} + {cfg.flows.name}")
    model = CrystalGenerationModule(
        network_name=cfg.networks.name,
        flow_name=cfg.flows.name,
        network_config=network_config,
        flow_config=flow_config,
        optimizer_config=optimizer_config,
        scheduler_config=scheduler_config,
    )
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    console.print(f"ğŸ“ˆ æ¨¡å‹å‚æ•°: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")
    console.print(f"   çº¦ {total_params/1e6:.1f}M å‚æ•°")
    
    # åˆ›å»ºæ•°æ®æ¨¡å—ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
    # å¦‚æœä½¿ç”¨diffcspç½‘ç»œï¼Œè‡ªåŠ¨å¯ç”¨GNNæ•°æ®åŠ è½½å™¨
    use_gnn_dataloader = (cfg.networks.name == 'diffcsp')
    
    data_module = CrystalGenerationDataModule(
        train_path=str(Path(orig_cwd) / cfg.data.train_path),
        val_path=str(Path(orig_cwd) / cfg.data.val_path),
        test_path=str(Path(orig_cwd) / cfg.data.test_path) if cfg.data.test_path else None,
        batch_size=cfg.data.batch_size,
        num_workers=cfg.data.num_workers,
        augment_permutation=cfg.data.augmentation.permutation.enabled,
        augment_so3=cfg.data.augmentation.so3.enabled,
        augment_prob=cfg.data.augmentation.permutation.prob,
        so3_augment_prob=cfg.data.augmentation.so3.prob,
        pin_memory=cfg.data.get('pin_memory', True),
        persistent_workers=cfg.data.get('persistent_workers', True),
        use_gnn_dataloader=use_gnn_dataloader,
        gnn_radius=cfg.networks.config.get('cutoff', 6.0) if use_gnn_dataloader else 6.0,
        gnn_max_neighbors=cfg.networks.config.get('max_neighbors', 50) if use_gnn_dataloader else 50,
    )
    
    # è®¾ç½®å›è°ƒå’Œæ—¥å¿—
    use_wandb = cfg.get('wandb', {}).get('enabled', False)
    callbacks = setup_callbacks(cfg, checkpoint_dir, use_wandb)
    loggers = setup_loggers(cfg, logs_dir)
    
    # å‡†å¤‡Trainerå‚æ•°
    trainer_kwargs = {
        'max_epochs': cfg.trainer.max_epochs,
        'gradient_clip_val': cfg.trainer.gradient_clip_val,
        'accumulate_grad_batches': cfg.trainer.accumulate_grad_batches,
        'val_check_interval': cfg.trainer.val_check_interval,
        'log_every_n_steps': cfg.trainer.log_every_n_steps,
        'callbacks': callbacks,
        'logger': loggers,
        'precision': cfg.trainer.precision,
        'deterministic': cfg.trainer.deterministic,
        'enable_checkpointing': True,
        'enable_progress_bar': True,
        'enable_model_summary': True,
    }
    
    # è®¾ç½®åŠ é€Ÿå™¨å’Œè®¾å¤‡
    if cfg.trainer.accelerator == 'gpu':
        if cfg.trainer.devices == -1:
            trainer_kwargs['accelerator'] = 'gpu'
            trainer_kwargs['devices'] = 'auto'
        else:
            trainer_kwargs['accelerator'] = 'gpu'
            trainer_kwargs['devices'] = cfg.trainer.devices
    else:
        trainer_kwargs['accelerator'] = 'cpu'
        trainer_kwargs['devices'] = 1
    
    # è®¾ç½®åˆ†å¸ƒå¼ç­–ç•¥
    if cfg.trainer.accelerator == 'gpu' and (cfg.trainer.devices == -1 or cfg.trainer.devices > 1):
        if torch.cuda.is_available():
            num_gpus = torch.cuda.device_count()
            if num_gpus > 1:
                console.print(f"ğŸš€ ä½¿ç”¨DDPç­–ç•¥ï¼Œ{num_gpus}ä¸ªGPU")
                trainer_kwargs['strategy'] = DDPStrategy(
                    find_unused_parameters=False,
                    gradient_as_bucket_view=True,
                )
            else:
                trainer_kwargs['strategy'] = 'auto'
    
    # è°ƒè¯•é€‰é¡¹
    if cfg.trainer.fast_dev_run:
        trainer_kwargs['fast_dev_run'] = True
        console.print("âš¡ å¿«é€Ÿå¼€å‘è¿è¡Œæ¨¡å¼ (1 batch)")
    
    if cfg.trainer.overfit_batches > 0:
        trainer_kwargs['overfit_batches'] = cfg.trainer.overfit_batches
        console.print(f"ğŸ”„ è¿‡æ‹Ÿåˆæµ‹è¯•: {cfg.trainer.overfit_batches} batches")
    
    # æ€§èƒ½åˆ†æ
    if cfg.trainer.profile:
        from pytorch_lightning.profilers import PyTorchProfiler
        trainer_kwargs['profiler'] = PyTorchProfiler(
            dirpath=logs_dir,
            filename="profiler_report",
        )
        console.print("ğŸ“Š å¯ç”¨PyTorchæ€§èƒ½åˆ†æ")
    
    # åˆ›å»ºTrainer
    trainer = pl.Trainer(**trainer_kwargs)
    
    # å¼€å§‹è®­ç»ƒ
    console.print("\n[bold green]ğŸš€ å¼€å§‹è®­ç»ƒ...[/bold green]")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ¢å¤ç‚¹
    resume_path = cfg.get('resume_from', None)
    if resume_path and Path(resume_path).exists():
        console.print(f"ğŸ“‚ ä»æ£€æŸ¥ç‚¹æ¢å¤: {resume_path}")
        trainer.fit(model, data_module, ckpt_path=resume_path)
    else:
        trainer.fit(model, data_module)
    
    # æµ‹è¯•ï¼ˆå¦‚æœæœ‰æµ‹è¯•é›†ï¼‰
    if cfg.data.test_path:
        console.print("\nğŸ§ª è¿è¡Œæµ‹è¯•...")
        trainer.test(model, data_module)
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    console.print("\n" + "="*60)
    
    # æŸ¥æ‰¾ModelCheckpointå›è°ƒä»¥è·å–æœ€ä½³æ¨¡å‹è·¯å¾„
    best_model_path = "N/A"
    for callback in callbacks:
        if isinstance(callback, ModelCheckpoint):
            best_model_path = getattr(callback, 'best_model_path', 'N/A')
            break
    
    console.print(Panel.fit(
        f"[bold green]âœ… è®­ç»ƒå®Œæˆï¼[/bold green]\n\n"
        f"ğŸ“ å®éªŒè¾“å‡º: {exp_dir}\n"
        f"ğŸ“Š æŒ‡æ ‡CSV: {logs_dir}/metrics/metrics.csv\n"
        f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model_path}",
        title="è®­ç»ƒæ€»ç»“",
        border_style="green"
    ))


if __name__ == '__main__':
    main()